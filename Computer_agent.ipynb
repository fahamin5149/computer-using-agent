{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN37x5iMl91YlMnCf50j3zX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fahamin5149/computer-using-agent/blob/main/Computer_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q llama_index llama-index-llms-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0_sWGpYZQLl",
        "outputId": "4ab0a6f0-f565-48d2-85ad-9329b3e377b5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.6/250.6 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.7/298.7 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install llama-index-vector-stores-qdrant llama-index-readers-file llama-index-embeddings-fastembed\n",
        "!pip install -q youtube-transcript-api langchain_community tiktoken langchain-openai langchainhub chromadb langchain langchain-core langchain_google_genai llama_index qdrant-client sentence-transformers fastembed llama-index-llms-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RgECA44JbSsf",
        "outputId": "84142666-e0d5-49b1-80ba-456884ff59ec"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index-vector-stores-qdrant\n",
            "  Downloading llama_index_vector_stores_qdrant-0.4.3-py3-none-any.whl.metadata (767 bytes)\n",
            "Requirement already satisfied: llama-index-readers-file in /usr/local/lib/python3.11/dist-packages (0.4.4)\n",
            "Collecting llama-index-embeddings-fastembed\n",
            "  Downloading llama_index_embeddings_fastembed-0.3.0-py3-none-any.whl.metadata (697 bytes)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.60.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-vector-stores-qdrant) (1.70.0)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.7 in /usr/local/lib/python3.11/dist-packages (from llama-index-vector-stores-qdrant) (0.12.15)\n",
            "Collecting qdrant-client>=1.7.1 (from llama-index-vector-stores-qdrant)\n",
            "  Downloading qdrant_client-1.13.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file) (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file) (2.2.2)\n",
            "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file) (5.2.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file) (0.0.26)\n",
            "Collecting fastembed>=0.2.2 (from llama-index-embeddings-fastembed)\n",
            "  Downloading fastembed-0.5.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file) (2.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.20 in /usr/local/lib/python3.11/dist-packages (from fastembed>=0.2.2->llama-index-embeddings-fastembed) (0.27.1)\n",
            "Collecting loguru<0.8.0,>=0.7.2 (from fastembed>=0.2.2->llama-index-embeddings-fastembed)\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting mmh3<5.0.0,>=4.1.0 (from fastembed>=0.2.2->llama-index-embeddings-fastembed)\n",
            "  Downloading mmh3-4.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.11/dist-packages (from fastembed>=0.2.2->llama-index-embeddings-fastembed) (1.26.4)\n",
            "Collecting onnxruntime!=1.20.0,>=1.17.0 (from fastembed>=0.2.2->llama-index-embeddings-fastembed)\n",
            "  Downloading onnxruntime-1.20.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting pillow<11.0.0,>=10.3.0 (from fastembed>=0.2.2->llama-index-embeddings-fastembed)\n",
            "  Downloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting py-rust-stemmers<0.2.0,>=0.1.0 (from fastembed>=0.2.2->llama-index-embeddings-fastembed)\n",
            "  Downloading py_rust_stemmers-0.1.3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: requests<3.0,>=2.31 in /usr/local/lib/python3.11/dist-packages (from fastembed>=0.2.2->llama-index-embeddings-fastembed) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from fastembed>=0.2.2->llama-index-embeddings-fastembed) (0.21.0)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.66 in /usr/local/lib/python3.11/dist-packages (from fastembed>=0.2.2->llama-index-embeddings-fastembed) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (3.11.11)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (3.9.1)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (2.10.6)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (9.0.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (0.8.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.17.2)\n",
            "Collecting grpcio-tools>=1.41.0 (from qdrant-client>=1.7.1->llama-index-vector-stores-qdrant)\n",
            "  Downloading grpcio_tools-1.70.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant-client>=1.7.1->llama-index-vector-stores-qdrant)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.11/dist-packages (from qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (2.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file) (2025.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.18.3)\n",
            "Collecting protobuf<6.0dev,>=5.26.1 (from grpcio-tools>=1.41.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant)\n",
            "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from grpcio-tools>=1.41.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (75.1.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (0.14.0)\n",
            "Collecting h2<5,>=3 (from httpx[http2]>=0.20.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant)\n",
            "  Downloading h2-4.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed>=0.2.2->llama-index-embeddings-fastembed) (3.17.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed>=0.2.2->llama-index-embeddings-fastembed) (24.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (2024.11.6)\n",
            "Collecting coloredlogs (from onnxruntime!=1.20.0,>=1.17.0->fastembed>=0.2.2->llama-index-embeddings-fastembed)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed>=0.2.2->llama-index-embeddings-fastembed) (25.1.24)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed>=0.2.2->llama-index-embeddings-fastembed) (1.13.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.31->fastembed>=0.2.2->llama-index-embeddings-fastembed) (3.4.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (3.26.1)\n",
            "Collecting hyperframe<7,>=6.1 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant)\n",
            "  Downloading hyperframe-6.1.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting hpack<5,>=4.1 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant)\n",
            "  Downloading hpack-4.1.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime!=1.20.0,>=1.17.0->fastembed>=0.2.2->llama-index-embeddings-fastembed)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime!=1.20.0,>=1.17.0->fastembed>=0.2.2->llama-index-embeddings-fastembed) (1.3.0)\n",
            "Downloading llama_index_vector_stores_qdrant-0.4.3-py3-none-any.whl (11 kB)\n",
            "Downloading llama_index_embeddings_fastembed-0.3.0-py3-none-any.whl (2.7 kB)\n",
            "Downloading fastembed-0.5.1-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qdrant_client-1.13.2-py3-none-any.whl (306 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.6/306.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio_tools-1.70.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-4.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.20.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Downloading py_rust_stemmers-0.1.3-cp311-cp311-manylinux_2_28_x86_64.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.9/323.9 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-4.2.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-4.1.0-py3-none-any.whl (34 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hyperframe-6.1.0-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: py-rust-stemmers, mmh3, protobuf, portalocker, pillow, loguru, hyperframe, humanfriendly, hpack, h2, grpcio-tools, coloredlogs, onnxruntime, qdrant-client, fastembed, llama-index-vector-stores-qdrant, llama-index-embeddings-fastembed\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.6\n",
            "    Uninstalling protobuf-4.25.6:\n",
            "      Successfully uninstalled protobuf-4.25.6\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.1.0\n",
            "    Uninstalling pillow-11.1.0:\n",
            "      Successfully uninstalled pillow-11.1.0\n",
            "Successfully installed coloredlogs-15.0.1 fastembed-0.5.1 grpcio-tools-1.70.0 h2-4.2.0 hpack-4.1.0 humanfriendly-10.0 hyperframe-6.1.0 llama-index-embeddings-fastembed-0.3.0 llama-index-vector-stores-qdrant-0.4.3 loguru-0.7.3 mmh3-4.1.0 onnxruntime-1.20.1 pillow-10.4.0 portalocker-2.10.1 protobuf-5.29.3 py-rust-stemmers-0.1.3 qdrant-client-1.13.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "b67f82b070cc4c7d8bab210deb0144a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.3/622.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.7/412.7 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from llama_index.llms.groq import Groq\n",
        "from llama_index.core import Document\n",
        "import uuid\n",
        "from llama_index.core.text_splitter import SentenceSplitter\n",
        "import logging\n",
        "import sys\n",
        "import os\n",
        "import qdrant_client\n",
        "from IPython.display import Markdown, display\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.core import StorageContext\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
        "from llama_index.core import Settings\n"
      ],
      "metadata": {
        "id": "HMSL1vqxd3Hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up LLM"
      ],
      "metadata": {
        "id": "JHmfYX1BZLtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kx6BwlAdyBP",
        "outputId": "ffd487cb-6f0f-4ed6-c76f-0b629057fb3d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n",
            "Downloading groq-0.18.0-py3-none-any.whl (121 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/121.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "aisXK2ECYp8g"
      },
      "outputs": [],
      "source": [
        "from groq import Groq\n",
        "import os\n",
        "\n",
        "\n",
        "client = Groq(\n",
        "    api_key=os.environ.get(\"groq_api\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Generate a numbered, ordered list of technical topics I should learn if I want to work on LLM\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"deepseek-r1-distill-llama-70b\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnQG0swEcQmU",
        "outputId": "83046d6a-d0e9-49c8-965c-313a2814859c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<think>\n",
            "Okay, so I want to work on large language models (LLMs), but I'm not exactly sure where to start. I've heard that LLMs are a big part of AI these days, and they're used for things like chatbots, translation, and text generation. But I'm a bit overwhelmed by all the technical terms and areas I need to learn. Let me try to break this down.\n",
            "\n",
            "First, I know that LLMs are a type of machine learning model, specifically deep learning. So, I guess I need to start with the basics of machine learning. That probably includes supervised and unsupervised learning. I've heard about neural networks, so I should look into that. Maybe I should understand how neural networks work, the different layers like input, hidden, and output layers. Also, activation functions and how they introduce non-linearity into the model.\n",
            "\n",
            "Wait, optimization techniques are also important. I remember hearing about gradient descent and backpropagation. These are methods used to train models, right? So I need to understand how those work. Maybe I should also look into different optimizers like Adam or SGD. Oh, and loss functions, because that's what the model tries to minimize during training.\n",
            "\n",
            "Then there's deep learning frameworks. I know TensorFlow and PyTorch are popular. I think PyTorch is more commonly used in research, so maybe I should focus on that. I should learn how to build models, train them, and use pre-trained models. Also, I need to understand how GPUs work with these frameworks because training models can be computationally intensive.\n",
            "\n",
            "Moving on to natural language processing (NLP), which is the area where LLMs are primarily applied. I think I should start with the basics of NLP. Tokenization is the process of breaking text into words or tokens, right? Then there's stop words removal, stemming, and lemmatization. Oh, and understanding embeddings like Word2Vec or GloVe, which convert words into vectors.\n",
            "\n",
            "Sequence models are next. I've heard of RNNs, LSTMs, and GRUs. RNNs are good for sequential data, but they have issues with long-term dependencies. LSTMs and GRUs are improvements over RNNs, so I should learn about their architectures and how they handle memory.\n",
            "\n",
            "Attention mechanisms are crucial for models like Transformers. I remember the Transformer model introduced self-attention, which allows the model to focus on different parts of the input when generating each part of the output. I need to understand how multi-head attention works and the concept of positional encoding since Transformers don't have recurrence.\n",
            "\n",
            "Pre-trained language models are a big part of LLMs. I should learn about models like BERT, RoBERTa, and GPT. Each has a different approach—BERT uses a masked language modeling approach, while GPT is autoregressive. Understanding how these models are trained and fine-tuned is important.\n",
            "\n",
            "For LLMs specifically, I need to know about their architecture. They're usually based on the Transformer model, so I should dive deeper into that. Scaling is another aspect—models have gotten larger, so understanding how model size affects performance and training is key.\n",
            "\n",
            "Training LLMs must require a lot of data and computational resources. I should learn about the datasets used, like the Common Crawl or BookCorpus. Data preprocessing is also important—how to clean and format the data for training. Distributed training is probably necessary for large models, so I need to understand how that works, maybe using libraries like Dask or joblib.\n",
            "\n",
            "Once the model is trained, evaluation metrics are needed. For LLMs, perplexity is a common metric, but I'm not exactly sure how it's calculated. I should also look into other metrics like BLEU or ROUGE, which are used for text generation tasks.\n",
            "\n",
            "Fine-tuning is the process of adapting a pre-trained model to a specific task. I need to learn how to do that effectively, maybe starting with smaller models before scaling up. Prompt engineering is something I've heard about recently, where you design prompts to guide the model's output. That seems important for getting the desired results from the model.\n",
            "\n",
            "Advanced topics like few-shot and zero-shot learning would be useful. Few-shot learning means the model can learn from a few examples, which is different from traditional machine learning approaches. Zero-shot learning is when the model can perform tasks it wasn't explicitly trained on, which sounds pretty advanced.\n",
            "\n",
            "Reinforcement learning from human feedback (RLHF) is another area. I think this involves using human feedback to fine-tune the model's outputs, making them more aligned with what users want. I need to understand how this process works and how it's integrated into training.\n",
            "\n",
            "Ethical considerations are crucial. I should learn about bias in models and how to mitigate it. Also, ensuring that the model doesn't generate harmful content is important. Environmental impact is another aspect, considering the energy required to train large models.\n",
            "\n",
            "Specialized areas within LLMs might interest me. For example, multimodal models that can handle text and images, or models designed for specific tasks like code generation or question answering. I should explore these areas if I have particular interests.\n",
            "\n",
            "Staying updated with the latest research is important. Reading papers from conferences like NeurIPS or ACL would help me keep up with advancements. Maybe joining communities or following experts on social media could provide additional insights and resources.\n",
            "\n",
            "Finally, building projects is a great way to apply what I've learned. Starting with simple models and gradually moving to more complex ones would help solidify my understanding. Contributing to open-source projects could also provide practical experience and expose me to real-world challenges.\n",
            "\n",
            "Wait, I should make sure I have a strong foundation in programming, especially Python, since it's widely used in machine learning. Also, understanding software engineering best practices will help when working on larger projects or contributing to existing ones.\n",
            "\n",
            "I think I've covered a lot of areas, but I might be missing some details. Maybe I should organize this into a structured list to make sure I don't forget anything important. Starting from the basics and moving to more advanced topics seems logical. I should also consider practical experience, like setting up a machine with the right tools and libraries, and experimenting with existing models to get a feel for how they work.\n",
            "\n",
            "Overall, it's a lot to learn, but breaking it down into manageable sections makes it less daunting. I can tackle each topic one by one, starting with the basics and gradually moving to more specialized areas. Practicing regularly and working on projects will help reinforce the concepts and prepare me for working on LLMs.\n",
            "</think>\n",
            "\n",
            "To work on Large Language Models (LLMs), follow this organized and structured approach:\n",
            "\n",
            "### 1. **Foundational Knowledge**\n",
            "   - **Machine Learning Basics**: Understand supervised and unsupervised learning, neural networks, activation functions, and optimization techniques like gradient descent and backpropagation.\n",
            "   - **Deep Learning Frameworks**: Focus on PyTorch for building, training, and using pre-trained models, and understand GPU utilization.\n",
            "\n",
            "### 2. **Natural Language Processing (NLP) Basics**\n",
            "   - **Text Preprocessing**: Learn tokenization, stop words removal, stemming, and lemmatization.\n",
            "   - **Word Embeddings**: Study Word2Vec, GloVe, and their applications.\n",
            "   - **Sequence Models**: Understand RNNs, LSTMs, GRUs, and their role in sequential data.\n",
            "   - **Attention Mechanisms**: Delve into self-attention, multi-head attention, and positional encoding.\n",
            "\n",
            "### 3. **Pre-trained Language Models**\n",
            "   - **Models Overview**: Learn about BERT, RoBERTa, and GPT, focusing on their training methods and fine-tuning.\n",
            "\n",
            "### 4. **Large Language Models (LLMs)**\n",
            "   - **Architecture**: Study the Transformer model and scaling aspects.\n",
            "   - **Training**: Understand data sources like Common Crawl, preprocessing, and distributed training.\n",
            "   - **Evaluation**: Familiarize yourself with perplexity, BLEU, and ROUGE.\n",
            "   - **Fine-tuning and Prompt Engineering**: Adapt models to specific tasks and design effective prompts.\n",
            "\n",
            "### 5. **Advanced Topics**\n",
            "   - **Few-shot and Zero-shot Learning**: Explore learning from minimal examples and unseen tasks.\n",
            "   - **Reinforcement Learning from Human Feedback (RLHF)**: Use human feedback to refine model outputs.\n",
            "   - **Ethical Considerations**: Address bias mitigation, harmful content prevention, and environmental impact.\n",
            "\n",
            "### 6. **Specialized Areas**\n",
            "   - **Multimodal Models**: Explore models handling text and images.\n",
            "   - **Task-specific Models**: Investigate models for code generation or question answering.\n",
            "\n",
            "### 7. **Community and Research**\n",
            "   - **Stay Updated**: Read research papers and join communities for the latest advancements.\n",
            "\n",
            "### 8. **Practical Experience**\n",
            "   - **Projects**: Start with simple models, progress to complex ones, and contribute to open-source projects.\n",
            "   - **Tools and Programming**: Ensure proficiency in Python and software engineering practices.\n",
            "\n",
            "By systematically addressing each area, you can build a comprehensive understanding and practical skills needed to work on LLMs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Hello! You are an assistant who will only reply in Roman urdu language\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Kia haal hai? Main aapki kaise madad kar sakta hoon?\"},\n",
        "        {\"role\": \"user\", \"content\": \"Create a well-organized routine for me\"}\n",
        "    ],\n",
        "    model=\"deepseek-r1-distill-llama-70b\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O62FXc8OcdDh",
        "outputId": "7e3c8d5e-5433-43b8-a00a-592b4b8faeed"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<think>\n",
            "Alright, the user just asked me to create a well-organized routine for them. They mentioned they want it in Roman Urdu, so I need to make sure I structure it clearly in that format. \n",
            "\n",
            "First, I should consider the key components of a daily routine. Usually, it starts from waking up, then exercise, breakfast, work or study, lunch, some leisure time, more work, evening activities, dinner, and then winding down before bed. \n",
            "\n",
            "I need to break down each part of the day into manageable chunks. Maybe allocate specific time slots so the routine is structured. Let's see, waking up at 6 AM is a good start, then exercise until 7. After that, breakfast and getting ready. \n",
            "\n",
            "For work or study, I can set aside a solid block in the morning. Lunch at 1 PM, then some rest. The afternoon can be for more work or studies. Leisure time in the evening, maybe around 5 PM. Dinner at 8 PM and some relaxation before bed at 10:30 PM. \n",
            "\n",
            "I should also include some tips to make the routine effective. Maybe advice on sticking to the schedule, taking short breaks, staying hydrated, eating healthy, exercising regularly, and getting enough sleep. \n",
            "\n",
            "I need to present all this in Roman Urdu, making sure it's easy to read and follow. Each time slot should be clear, and the tips should be concise but helpful. \n",
            "\n",
            "Let me organize this into sections: Morning, Day, Evening, Night, and then the tips. That way, it's easy for the user to follow. I'll use bullet points or numbers where necessary to enhance readability. \n",
            "\n",
            "I should also make sure the language is simple and straightforward, avoiding any complicated words since the user might not be highly fluent in English. \n",
            "\n",
            "Double-checking the time allocations to ensure they are realistic and allow for a balanced day. It's important the routine isn't too rigid but still provides structure. \n",
            "\n",
            "Including elements like leisure time and relaxation is crucial to maintain a healthy work-life balance. Maybe suggest activities like reading or walking in the evening. \n",
            "\n",
            "Alright, putting it all together now, ensuring each part flows logically and covers all aspects of the day. Making sure the Roman Urdu is accurate and easy to understand. \n",
            "\n",
            "I think that's a solid plan. Time to draft the response accordingly.\n",
            "</think>\n",
            "\n",
            "Yeh raha aapka well-organized daily routine (Roman Urdu mein):\n",
            "\n",
            "---\n",
            "\n",
            "### **Morning Routine (Subah Ki Routine):**\n",
            "1. **6:00 AM - Wake Up (Jagna):**\n",
            "   - Bed se uthkar duaa karein.\n",
            "   - Morning walk ya exercise karein (30 minutes).\n",
            "\n",
            "2. **6:30 AM - Fresh Up (Tayyar Hona):**\n",
            "   - Brush karein, face wash karein, aur naha dhokar fresh ho jayein.\n",
            "\n",
            "3. **7:00 AM - Breakfast (Nashta):**\n",
            "   - Healthy breakfast karein (jaise poha, omelette, ya phal).\n",
            "\n",
            "4. **7:30 AM - Get Ready (Tayyar Hona):**\n",
            "   - Office ya studies ke liye tayyar ho jayein.\n",
            "\n",
            "---\n",
            "\n",
            "### **Day Routine (Din Ki Routine):**\n",
            "5. **8:00 AM - Start Work/Study (Kaam ya Padhai Shuru Karein):**\n",
            "   - Apne goals ko achieve karne ke liye focused rahein.\n",
            "   - Short breaks lein (5-10 minutes) har 1 ghante ke baad.\n",
            "\n",
            "6. **1:00 PM - Lunch (Dopahar Ka Khana):**\n",
            "   - Healthy aur poora khana karein.\n",
            "   - 10-15 minutes ka rest lein.\n",
            "\n",
            "7. **1:30 PM - Resume Work/Study (Kaam ya Padhai Dobara Shuru Karein):**\n",
            "   - Agla session focused aur productive banayein.\n",
            "\n",
            "8. **5:00 PM - Evening Break (Sham Ki Chai):**\n",
            "   - Chai ya coffee karein, aur thoda samay relax karein.\n",
            "\n",
            "---\n",
            "\n",
            "### **Evening Routine (Sham Ki Routine):**\n",
            "9. **5:30 PM - Leisure Time (Aaram Karne Ka Samay):**\n",
            "   - Kuchh samay apne shauk ke liye nikaalein (jaise kitaab padhna, gaana sunna, ya walk karna).\n",
            "\n",
            "10. **6:30 PM - Exercise/Yoga (Vyaayam ya Yoga):**\n",
            "    - Rozana 30-45 minutes exercise karein.\n",
            "\n",
            "11. **7:30 PM - Dinner (Shaam Ka Khana):**\n",
            "    - Healthy aur halka khana karein.\n",
            "\n",
            "---\n",
            "\n",
            "### **Night Routine (Raat Ki Routine):**\n",
            "12. **8:00 PM - Relaxation Time (Aaram Karne Ka Samay):**\n",
            "    - Family ya doston ke saath samay bitayein.\n",
            "    - TV dekhna ya koi hobby pursue karein.\n",
            "\n",
            "13. **9:30 PM - Wind Down (Din Bhar Ka End):**\n",
            "    - Apne din ki planning aur goals review karein.\n",
            "    - Kal ke liye plan banayein.\n",
            "\n",
            "14. **10:00 PM - Prepare for Bed (Bistar Tyaar Karein):**\n",
            "    - Light reading ya meditation karein.\n",
            "\n",
            "15. **10:30 PM - Sleep (Sone Jaayein):**\n",
            "    - Puray 7-8 ghante ki neend lein.\n",
            "\n",
            "---\n",
            "\n",
            "### **Tips for a Successful Routine:**\n",
            "- Har din is routine ka strictly paalan karein.\n",
            "- Short breaks lete rahein taaki dimaag fresh rahe.\n",
            "- Paani peena aur healthy khana khayein.\n",
            "- Exercise ko apni routine ka hissa banayein.\n",
            "- Neend ka pura dhyan rakhein.\n",
            "\n",
            "Umeed hai yeh routine aapko organized aur productive banayegi! 😊\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Max Token"
      ],
      "metadata": {
        "id": "rjdNaqM_i8Pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Generate a numbered, ordered list of technical topics I should learn if I want to work on LLM\",\n",
        "        }\n",
        "    ],\n",
        "    max_tokens=100,\n",
        "    model=\"deepseek-r1-distill-llama-70b\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtTwFpy9i66T",
        "outputId": "2f707214-eeeb-42a4-b803-fe0bb1dace7c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<think>\n",
            "Okay, so I want to work on Large Language Models (LLMs), but I'm not really sure where to start. I know that LLMs are a big part of AI these days, but the field seems really vast. Let me try to break this down.\n",
            "\n",
            "First, I remember that LLMs are built using machine learning, so I probably need to get a solid foundation in that. But wait, machine learning itself has a lot of areas. There's supervised learning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion.choices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJHqwPlqflVJ",
        "outputId": "f287da2b-4647-4ca2-8add-32d3db601b89"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content=\"<think>\\nOkay, so I want to work on Large Language Models (LLMs), but I'm not really sure where to start. I know that LLMs are a big part of AI these days, but the field seems really vast. Let me try to break this down.\\n\\nFirst, I remember that LLMs are built using machine learning, so I probably need to get a solid foundation in that. But wait, machine learning itself has a lot of areas. There's supervised learning,\", role='assistant', function_call=None, reasoning=None, tool_calls=None))]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Temperature\n",
        "Higher temperature means more creativity"
      ],
      "metadata": {
        "id": "kSsrjha-jPmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Generate a numbered, ordered list of technical topics I should learn if I want to work on LLM\",\n",
        "        }\n",
        "    ],\n",
        "    temperature=0.7,\n",
        "    model=\"deepseek-r1-distill-llama-70b\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtD18EiqjF8F",
        "outputId": "78a971fb-7912-46d8-b962-b28e34f481fa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<think>\n",
            "Okay, so I want to work on Large Language Models (LLMs), but I'm not exactly sure where to start. I've heard a lot about machine learning and AI, but the specifics are a bit overwhelming. Let me try to break this down step by step.\n",
            "\n",
            "First, I know that LLMs are a type of AI, so I probably need to understand the basics of machine learning. I remember hearing about supervised and unsupervised learning. Supervised learning is where the model is trained on labeled data, right? Like, each example has an input and the correct output. Unsupervised learning is where the model finds patterns in unlabeled data. But I'm not entirely clear on how that applies to language models. Maybe in NLP, unsupervised learning is used for things like clustering texts or finding topics?\n",
            "\n",
            "Then there's deep learning. I think that's a subset of machine learning that uses neural networks with many layers. Neural networks are inspired by the human brain, with nodes (neurons) connected in layers. I should probably learn about different types of neural networks, like CNNs and RNNs. Wait, CNNs are Convolutional Neural Networks, mainly used for images, right? But RNNs are Recurrent Neural Networks, which are good for sequences like text or time series data. Oh, and LSTMs are a type of RNN that helps with long-term dependencies, which is useful for longer texts.\n",
            "\n",
            "Transformer architectures are a big deal in LLMs. I've heard of BERT, RoBERTa, and GPT models. Transformers use self-attention mechanisms, which allow the model to weigh the importance of different words in a sentence. I should definitely dive deeper into how transformers work, maybe starting with the original Transformer paper.\n",
            "\n",
            "Moving on to NLP fundamentals. I know tokenization is breaking text into words or subwords. But what's the difference between tokens and subwords? I think subwords are smaller units, like parts of words, to handle rare or unseen words better. Part-of-speech tagging is identifying nouns, verbs, etc., but I'm not sure how that's used in LLMs. Maybe for feature extraction or understanding sentence structure.\n",
            "\n",
            "Named Entity Recognition (NER) is about identifying entities like names and places. Dependency parsing is about sentence structure, like subject-verb-object relationships. Coreference resolution is figuring out what pronouns refer to, which is important for understanding context in longer texts.\n",
            "\n",
            "Pretrained models are a cornerstone of modern NLP. Models like BERT are pretrained on large datasets to learn language representations. Fine-tuning is taking these models and adapting them to specific tasks, like sentiment analysis or question answering. I think transfer learning is important here because it allows using knowledge from one task on another, which is efficient.\n",
            "\n",
            "Language model architecture involves understanding how the model is structured. I need to know about encoder-decoder models, like the original Transformer, where the encoder processes input and the decoder generates output. Training objectives are like the goals the model is trained on. Masked language modeling is where some words are hidden and the model predicts them, which is how BERT is trained. Next sentence prediction is predicting if two sentences are adjacent, which helps in understanding relationships between sentences.\n",
            "\n",
            "Optimization techniques are crucial for training these large models. Adam and AdamW are optimization algorithms that adapt learning rates for each parameter. Learning rate scheduling is adjusting the learning rate during training to improve convergence. Gradient clipping is limiting gradient values to prevent exploding gradients, which can destabilize training.\n",
            "\n",
            "Regularization techniques prevent overfitting. Dropout randomly deactivates neurons during training to stop the model from relying too much on any single neuron. Weight decay adds a penalty term to the loss to discourage large weights. Layer normalization normalizes the activations of each layer to stabilize training.\n",
            "\n",
            "Advanced topics include attention mechanisms beyond the standard self-attention. Maybe there are variations like sparse attention or other efficiency improvements. Model parallelism is about splitting the model across multiple GPUs or machines because LLMs are huge. Pipelining is processing different parts of the model in stages to improve computation efficiency.\n",
            "\n",
            "Efficient inference is important for deploying models. Quantization reduces the precision of model weights to make the model smaller and faster. Pruning removes unnecessary weights or neurons to reduce model size. Knowledge distillation is teaching a smaller model (student) to mimic a larger model (teacher) to maintain performance while reducing size.\n",
            "\n",
            "Multilingual models are trained on multiple languages, so they can understand and generate text in different languages. Few-shot and zero-shot learning are about the model's ability to perform tasks with few or no examples, which is pretty cool.\n",
            "\n",
            "Specialized models include multimodal models that handle text and images together, like CLIP or Flamingo. Instruction-tuned models are optimized to follow instructions, like GPT-3.5 or PaLM, which can perform specific tasks better.\n",
            "\n",
            "For deployment, I need to know about model serving, which is making the model available as an API. Rate limiting is controlling how many requests are handled to prevent abuse or overuse. Monitoring is tracking the model's performance and usage in real-time. A/B testing is comparing different model versions to see which performs better.\n",
            "\n",
            "Ethics, fairness, and safety are important to ensure models don't perpetuate biases or harm users. I need to understand bias mitigation strategies and how to make models more transparent and explainable. Safety measures prevent the model from generating harmful content.\n",
            "\n",
            "Staying updated is crucial in this fast-moving field. Following research papers, joining communities, and participating in competitions can help me keep up with the latest developments.\n",
            "\n",
            "The tools and frameworks are a bit intimidating. TensorFlow and PyTorch are the main deep learning frameworks. PyTorch is more flexible, while TensorFlow has more production tools. Hugging Face Transformers is a popular library for NLP. ONNX is for model conversion and optimization. Weights & Biases and TensorBoard are for tracking experiments and visualizing training metrics.\n",
            "\n",
            "Cloud platforms like AWS, GCP, and Azure provide infrastructure for training and deploying models. Docker and Kubernetes are for containerization and orchestration, which help in managing deployments.\n",
            "\n",
            "Math and theory are the foundation. Linear algebra is about vectors and matrices, which are essential for neural networks. Calculus is needed for optimization and understanding how models learn. Probability and statistics are crucial for understanding distributions and uncertainties in data. Information theory explains concepts like entropy and cross-entropy, which are used in loss functions.\n",
            "\n",
            "The suggested learning path makes sense: start with ML basics, then NLP and deep learning, move to advanced topics, and finally deployment and ethics. It's a logical progression from foundation to application.\n",
            "\n",
            "I think I need to prioritize these topics, maybe start with the basics of ML and NLP, then move into deep learning and transformers. As I get comfortable, I can dive into more advanced areas like model parallelism and efficient inference. Deployment and ethics should be considered from the start to ensure responsible AI practices.\n",
            "\n",
            "I might be missing some areas or overcomplicating others, but this seems like a solid starting point. I should create a study plan, maybe using online courses and hands-on projects to apply what I learn. Practicing with real-world datasets and contributing to open-source projects could also provide valuable experience.\n",
            "</think>\n",
            "\n",
            "To work on Large Language Models (LLMs), follow this organized and prioritized learning path:\n",
            "\n",
            "### 1. **Foundations of Machine Learning**\n",
            "   - **Supervised & Unsupervised Learning**: Understand the basics, including applications in NLP.\n",
            "   - **Deep Learning**: Study neural networks, focusing on CNNs, RNNs, and LSTMs.\n",
            "\n",
            "### 2. **Transformer Architecture**\n",
            "   - **Transformers & Self-Attention**: Delve into the original Transformer paper and mechanisms like self-attention.\n",
            "   - **BERT, RoBERTa, GPT**: Explore these models and their architectures.\n",
            "\n",
            "### 3. **NLP Fundamentals**\n",
            "   - **Tokenization & Subwords**: Learn about breaking text into tokens and subwords.\n",
            "   - **Part-of-Speech Tagging, NER, Dependency Parsing, Coreference Resolution**: Understand these tasks and their roles in NLP.\n",
            "\n",
            "### 4. **Pretrained Models & Transfer Learning**\n",
            "   - **Pretraining & Fine-tuning**: Use models like BERT and apply transfer learning for specific tasks.\n",
            "\n",
            "### 5. **Language Model Architecture & Training**\n",
            "   - **Encoder-Decoder Models**: Study the Transformer architecture.\n",
            "   - **Training Objectives**: Explore masked language modeling and next sentence prediction.\n",
            "\n",
            "### 6. **Optimization & Regularization**\n",
            "   - **Optimization Techniques**: Learn Adam, AdamW, learning rate scheduling, and gradient clipping.\n",
            "   - **Regularization**: Understand dropout, weight decay, and layer normalization.\n",
            "\n",
            "### 7. **Advanced Topics**\n",
            "   - **Attention Mechanisms**: Explore beyond standard self-attention.\n",
            "   - **Model Parallelism & Pipelining**: Learn to split models across GPUs.\n",
            "   - **Efficient Inference**: Study quantization, pruning, and knowledge distillation.\n",
            "\n",
            "### 8. **Multilingual, Few-shot, Zero-shot Learning**\n",
            "   - **Multilingual Models**: Understand models trained on multiple languages.\n",
            "   - **Few-shot & Zero-shot Learning**: Learn task performance with minimal examples.\n",
            "\n",
            "### 9. **Specialized Models**\n",
            "   - **Multimodal Models**: Study models like CLIP for text-image tasks.\n",
            "   - **Instruction-Tuned Models**: Explore models optimized for instructions.\n",
            "\n",
            "### 10. **Deployment & Monitoring**\n",
            "   - **Model Serving & API**: Deploy models and manage requests.\n",
            "   - **Rate Limiting & Monitoring**: Control and track model usage and performance.\n",
            "   - **A/B Testing**: Compare model versions for effectiveness.\n",
            "\n",
            "### 11. **Ethics, Fairness, Safety**\n",
            "   - **Bias Mitigation & Transparency**: Ensure models are fair and explainable.\n",
            "   - **Safety Measures**: Prevent harmful content generation.\n",
            "\n",
            "### 12. **Stay Updated**\n",
            "   - **Research & Community**: Follow papers, join communities, and participate in competitions.\n",
            "\n",
            "### 13. **Tools & Frameworks**\n",
            "   - **TensorFlow, PyTorch, Hugging Face**: Use these for deep learning and NLP.\n",
            "   - **ONNX, Docker, Kubernetes**: Optimize and manage model deployments.\n",
            "   - **Cloud Platforms**: Utilize AWS, GCP, Azure for infrastructure.\n",
            "\n",
            "### 14. **Math & Theory**\n",
            "   - **Linear Algebra, Calculus, Probability, Statistics, Information Theory**: Build a strong foundation.\n",
            "\n",
            "### Suggested Learning Path\n",
            "1. **Start with ML Basics**: Cover supervised/unsupervised learning and deep learning.\n",
            "2. **Dive into NLP & Transformers**: Explore fundamentals and architectures.\n",
            "3. **Advanced Topics**: Tackle parallelism, efficient inference, and specialized models.\n",
            "4. **Deployment & Ethics**: Ensure responsible AI practices from the start.\n",
            "\n",
            "### Action Plan\n",
            "- **Study Plan**: Use online courses and hands-on projects.\n",
            "- **Practice**: Work with real-world datasets and contribute to open-source projects.\n",
            "\n",
            "This structured approach will help you build a strong foundation and progress effectively in working with LLMs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Vision model"
      ],
      "metadata": {
        "id": "TGPr-eWA0gcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "import base64\n",
        "\n",
        "\n",
        "# Function to encode the image\n",
        "def encode_image(image_path):\n",
        "  with open(image_path, \"rb\") as image_file:\n",
        "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "# Path to your image\n",
        "image_path = \"sf.png\"\n",
        "\n",
        "# Getting the base64 string\n",
        "base64_image = encode_image(image_path)\n",
        "\n",
        "client = Groq()\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\n",
        "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
        "                    },\n",
        "                },\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama-3.2-90b-vision-preview\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tt8mUBDkoyXs",
        "outputId": "b172a461-70b2-4cab-a2fe-bfd9fd59136a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The image depicts a table with various food containers, likely from a restaurant or catering service. The containers are arranged in two stacks: one stack of rectangular containers with clear lids and another stack of smaller, square containers with white lids. In the background, there are several black chairs and a table setting with utensils.\n",
            "\n",
            "Here is a detailed description of the items in the image:\n",
            "\n",
            "*   **Rectangular Containers**\n",
            "    *   Clear lids\n",
            "    *   Stacked on top of each other\n",
            "    *   Contain food\n",
            "*   **Square Containers**\n",
            "    *   White lids\n",
            "    *   Smaller than the rectangular containers\n",
            "    *   Also contain food\n",
            "*   **Chairs**\n",
            "    *   Black\n",
            "    *   Arranged around the table\n",
            "*   **Table Setting**\n",
            "    *   Utensils (forks, knives, spoons)\n",
            "    *   Plates (not visible)\n",
            "\n",
            "Overall, the image suggests that the food is prepared for takeout or delivery, as evidenced by the use of plastic containers. The presence of utensils and plates in the background implies that the food is intended to be eaten immediately, rather than stored for later consumption.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your image\n",
        "image_path = \"plant.png\"\n",
        "\n",
        "# Getting the base64 string\n",
        "base64_image = encode_image(image_path)\n",
        "\n",
        "client = Groq()\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": \"What species is this?\"},\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\n",
        "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
        "                    },\n",
        "                },\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama-3.2-90b-vision-preview\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyR-OPtx3BdD",
        "outputId": "dd5ffc7f-cfe4-4885-b782-9877fb320fc3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A carnivorous plant belonging to the family Nepenthaceae, native to tropical regions of Asia, including Malaysia, Indonesia, and the Philippines.\n",
            "\n",
            "**Characteristics:**\n",
            "\n",
            "• Modified leaves that form a deep, slippery cup to trap and digest insects and other small animals\n",
            "• Carnivorous behavior utilizes complex mechanisms to capture and digest prey\n",
            "• Striking appearance, with a range of colors and patterns on the pitchers\n",
            "\n",
            "**Habitat:**\n",
            "\n",
            "• Thrives in high-humidity environments with abundant sunlight\n",
            "• Often found in:\n",
            "  • Tropical forests\n",
            "  • Swamps\n",
            "  • Mountainous regions\n",
            "\n",
            "**Importance:**\n",
            "\n",
            "• Unique, fascinating plant with a specialized feeding mechanism\n",
            "• Widely collected and cultivated by enthusiasts around the world\n",
            "• Plays a vital role in tropical ecosystems, providing nutrition to animals in nutrient-poor environments\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_image(image_path):\n",
        "  with open(image_path, \"rb\") as image_file:\n",
        "    image_url=base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "    return {\n",
        "              \"type\": \"image_url\",\n",
        "              \"image_url\": {\n",
        "              \"url\": f\"data:image/jpeg;base64,{image_url}\",\n",
        "              },\n",
        "            }"
      ],
      "metadata": {
        "id": "of1W954H58fH"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your image\n",
        "image_path = \"invoice.png\"\n",
        "\n",
        "# Getting the base64 string\n",
        "base64_image = encode_image(image_path)\n",
        "\n",
        "client = Groq()\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\":\n",
        "                 \"\"\"\n",
        "                Generate a JSON object representing the contents\n",
        "                of this invoice.  It should include all dates,\n",
        "                dollar amounts, and addresses.\n",
        "                Only respond with the JSON itself.\n",
        "                 \"\"\"\n",
        "\n",
        "                 },\n",
        "                encode_image(image_path),\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama-3.2-90b-vision-preview\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldjzRNPz34vT",
        "outputId": "56b81f73-477d-4b6e-fa4c-17d81cf1700a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```\n",
            "{\n",
            "  \"invoice_number\": \"INV-2024-0042\",\n",
            "  \"invoice_date\": \"March 17, 2025\",\n",
            "  \"due_date\": \"April 16, 2025\",\n",
            "  \"bill_to\": {\n",
            "    \"name\": \"ACME Corporation\",\n",
            "    \"address\": {\n",
            "      \"street\": \"789 Market Street, Suite 500\",\n",
            "      \"city\": \"Los Angeles\",\n",
            "      \"state\": \"CA\",\n",
            "      \"zip\": \"90015\"\n",
            "    }\n",
            "  },\n",
            "  \"description\": [\n",
            "    {\n",
            "      \"quantity\": 1,\n",
            "      \"unit_price\": 5000.00,\n",
            "      \"amount\": 5000.00,\n",
            "      \"description\": \"Enterprise Software License\"\n",
            "    },\n",
            "    {\n",
            "      \"quantity\": 40,\n",
            "      \"unit_price\": 150.00,\n",
            "      \"amount\": 6000.00,\n",
            "      \"description\": \"Implementation Services\"\n",
            "    },\n",
            "    {\n",
            "      \"quantity\": 1,\n",
            "      \"unit_price\": 2500.00,\n",
            "      \"amount\": 2500.00,\n",
            "      \"description\": \"Premium Support Plan (Annual)\"\n",
            "    }\n",
            "  ],\n",
            "  \"subtotal\": 13500.00,\n",
            "  \"tax\": 1147.50,\n",
            "  \"total\": 14647.50\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"How does open source companies work\"\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama-3.2-90b-vision-preview\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRv4lwpN1BrC",
        "outputId": "6d1d260c-5bcc-469c-864e-99fbf3e47cf4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Open-source companies are organizations that develop and maintain software using the open-source model, which allows users to access, modify, and distribute the software for free. Here's a general overview of how open-source companies work:\n",
            "\n",
            "**Key characteristics:**\n",
            "\n",
            "1. **Free software**: Open-source companies provide their software for free, or at a minimal cost.\n",
            "2. **Open-source code**: The source code of the software is made available to the public, allowing users to review, modify, and distribute it.\n",
            "3. **Community-driven**: Open-source companies often rely on a community of users, developers, and contributors to help maintain and improve the software.\n",
            "\n",
            "**Revenue streams:**\n",
            "\n",
            "1. **Support and services**: Many open-source companies offer support, consulting, and training services to users, generating revenue from these services.\n",
            "2. **Licensing fees**: Some open-source companies charge fees for commercial use of their software, while still making the software available for free to individuals and non-profit organizations.\n",
            "3. **Donations**: Some open-source companies rely on donations to fund their development and maintenance efforts.\n",
            "4. **Sponsored features**: Some open-source companies offer sponsored features or plugins, which are developed and maintained by the company for specific clients or partners.\n",
            "5. **Platform and hosting**: Some open-source companies generate revenue by offering hosting services for their software or related platforms.\n",
            "\n",
            "**Models for open-source companies:**\n",
            "\n",
            "1. **Dual-licensing model**: Companies offer their software under an open-source license, while also providing a commercial license for proprietary use.\n",
            "2. **Service-based model**: Companies provide support, consulting, and training services to users, while also offering their software for free.\n",
            "3. **Foundation-based model**: Companies establish a non-profit foundation to manage and maintain the open-source software, while generating revenue through support and services.\n",
            "4. **Hybrid model**: Companies combine elements of the dual-licensing, service-based, and foundation-based models to generate revenue.\n",
            "\n",
            "**Examples of open-source companies:**\n",
            "\n",
            "1. **Red Hat**: Develops and supports open-source software, including Linux, and generates revenue through support and services.\n",
            "2. **Canonical**: Develops and supports the open-source Linux distribution Ubuntu, and generates revenue through support and services.\n",
            "3. **Appium**: Develops and maintains the open-source mobile app testing framework Appium, and generates revenue through support and training services.\n",
            "4. **Odoo**: Develops and maintains the open-source enterprise resource planning (ERP) software Odoo, and generates revenue through consulting, support, and hosting services.\n",
            "\n",
            "**Benefits and challenges:**\n",
            "\n",
            "**Benefits:**\n",
            "\n",
            "1. **Community engagement**: Open-source companies can tap into a community of developers and users, driving innovation and collaboration.\n",
            "2. **Rapid development**: Open-source companies can accelerate development by leveraging a community of contributors.\n",
            "3. **Increased adoption**: Open-source companies can reach a wider audience, as users are more likely to adopt free and open software.\n",
            "\n",
            "**Challenges:**\n",
            "\n",
            "1. **Revenue generation**: Open-source companies must find innovative ways to generate revenue, as traditional licensing fees are not applicable.\n",
            "2. **Maintenance and support**: Open-source companies must invest in maintenance and support to ensure the quality and reliability of their software.\n",
            "3. **Intellectual property**: Open-source companies must carefully manage intellectual property rights to ensure the integrity of their software.\n",
            "\n",
            "In summary, open-source companies work by providing free software, relying on community engagement, and generating revenue through support and services, licensing fees, donations, and platform and hosting services. They use various models, such as the dual-licensing model, service-based model, foundation-based model, and hybrid model, to achieve this.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using stream which will speed up the call to first token"
      ],
      "metadata": {
        "id": "ozkYic2W74_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"How does open source companies work\"\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama-3.2-90b-vision-preview\",\n",
        "    stream=True,\n",
        ")\n",
        "\n",
        "for chunk in chat_completion:\n",
        "    if chunk.choices and chunk.choices[0].delta.content:\n",
        "        print(chunk.choices[0].delta.content, end=\"\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWblRkDY1x_i",
        "outputId": "dbc36588-625f-467d-9cd7-f90531b63699"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Open-source companies operate on a unique business model that combines free and open-source software (FOSS) principles with commercial goals. Here's an overview of how they work:\n",
            "\n",
            "**Key characteristics:**\n",
            "\n",
            "1. **Free and open-source software**: Open-source companies develop and release software under an open-source license, which allows users to use, modify, and distribute the software freely.\n",
            "2. **Community-driven**: Open-source companies often rely on a community of volunteer developers, contributors, and users to contribute to the software's development, testing, and maintenance.\n",
            "3. **Transparent development**: Open-source companies typically use open-source collaboration tools and platforms, such as GitHub, to manage their development process, making their codebase and development history publicly visible.\n",
            "4. **Commercial support**: While the software is free, open-source companies offer commercial support, services, and features to generate revenue.\n",
            "\n",
            "**Revenue models:**\n",
            "\n",
            "1. **Support and services**: Open-source companies offer various levels of support, such as bug fixing, troubleshooting, and consulting services, for a fee.\n",
            "2. **Licensing**: Some open-source companies offer proprietary licenses to large enterprises, which includes additional features, support, or customization.\n",
            "3. **Hosting and subscriptions**: Some open-source companies offer hosting services or subscription-based models for access to their software, support, and updates.\n",
            "4. **Training and education**: Open-source companies can offer training, workshops, or online courses to educate users about their software and generate revenue.\n",
            "5. **Donations**: Some open-source companies rely on donations from their user community to support their development efforts.\n",
            "\n",
            "**Examples:**\n",
            "\n",
            "1. **Red Hat**: A leading open-source software company that offers Linux operating systems, middleware, and storage solutions, with a focus on enterprise customers.\n",
            "2. **Canonical (Ubuntu)**: A UK-based company behind the popular Ubuntu Linux operating system, which generates revenue through support services, consulting, and training.\n",
            "3. **MySQL**: An open-source relational database management system (RDBMS) company that was acquired by Oracle, but continues to offer support and services.\n",
            "4. **Apache Software Foundation**: A non-profit organization that develops and maintains various open-source projects, including the Apache HTTP Server, with a reliance on donations.\n",
            "\n",
            "**Benefits:**\n",
            "\n",
            "1. **Community involvement**: Open-source companies promote collaboration and community engagement, fostering innovation and faster development cycles.\n",
            "2. **Cost-effective**: Users can access high-quality software without the licensing costs associated with proprietary software.\n",
            "3. **Flexibility**: Open-source software can be customized to meet specific needs and use cases.\n",
            "4. **Security**: Open-source software can benefit from the scrutiny of a large community, reducing the risk of vulnerabilities.\n",
            "\n",
            "**Challenges:**\n",
            "\n",
            "1. **Revenue generation**: Balancing revenue needs with the principles of open-source software can be challenging.\n",
            "2. **Quality control**: Ensuring the quality and reliability of community-driven software can be difficult.\n",
            "3. **Maintaining a balance**: Walking the line between open-source principles and commercial interests can be a delicate balance.\n",
            "\n",
            "Overall, open-source companies have disrupted traditional software business models by providing high-quality software at no upfront cost. While they face challenges, these companies have proven that it's possible to build profitable businesses around open-source software."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WZw8y5Ie7EFE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}