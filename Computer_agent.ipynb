{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsJQsl/2HQTkKc/I2k9kOq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fahamin5149/computer-using-agent/blob/main/Computer_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q llama_index llama-index-llms-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0_sWGpYZQLl",
        "outputId": "4ab0a6f0-f565-48d2-85ad-9329b3e377b5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.6/250.6 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.7/298.7 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install llama-index-vector-stores-qdrant llama-index-readers-file llama-index-embeddings-fastembed\n",
        "!pip install -q youtube-transcript-api langchain_community tiktoken langchain-openai langchainhub chromadb langchain langchain-core langchain_google_genai llama_index qdrant-client sentence-transformers fastembed llama-index-llms-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RgECA44JbSsf",
        "outputId": "84142666-e0d5-49b1-80ba-456884ff59ec"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index-vector-stores-qdrant\n",
            "  Downloading llama_index_vector_stores_qdrant-0.4.3-py3-none-any.whl.metadata (767 bytes)\n",
            "Requirement already satisfied: llama-index-readers-file in /usr/local/lib/python3.11/dist-packages (0.4.4)\n",
            "Collecting llama-index-embeddings-fastembed\n",
            "  Downloading llama_index_embeddings_fastembed-0.3.0-py3-none-any.whl.metadata (697 bytes)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.60.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-vector-stores-qdrant) (1.70.0)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.7 in /usr/local/lib/python3.11/dist-packages (from llama-index-vector-stores-qdrant) (0.12.15)\n",
            "Collecting qdrant-client>=1.7.1 (from llama-index-vector-stores-qdrant)\n",
            "  Downloading qdrant_client-1.13.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file) (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file) (2.2.2)\n",
            "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file) (5.2.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file) (0.0.26)\n",
            "Collecting fastembed>=0.2.2 (from llama-index-embeddings-fastembed)\n",
            "  Downloading fastembed-0.5.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file) (2.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.20 in /usr/local/lib/python3.11/dist-packages (from fastembed>=0.2.2->llama-index-embeddings-fastembed) (0.27.1)\n",
            "Collecting loguru<0.8.0,>=0.7.2 (from fastembed>=0.2.2->llama-index-embeddings-fastembed)\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting mmh3<5.0.0,>=4.1.0 (from fastembed>=0.2.2->llama-index-embeddings-fastembed)\n",
            "  Downloading mmh3-4.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.11/dist-packages (from fastembed>=0.2.2->llama-index-embeddings-fastembed) (1.26.4)\n",
            "Collecting onnxruntime!=1.20.0,>=1.17.0 (from fastembed>=0.2.2->llama-index-embeddings-fastembed)\n",
            "  Downloading onnxruntime-1.20.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting pillow<11.0.0,>=10.3.0 (from fastembed>=0.2.2->llama-index-embeddings-fastembed)\n",
            "  Downloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting py-rust-stemmers<0.2.0,>=0.1.0 (from fastembed>=0.2.2->llama-index-embeddings-fastembed)\n",
            "  Downloading py_rust_stemmers-0.1.3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: requests<3.0,>=2.31 in /usr/local/lib/python3.11/dist-packages (from fastembed>=0.2.2->llama-index-embeddings-fastembed) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from fastembed>=0.2.2->llama-index-embeddings-fastembed) (0.21.0)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.66 in /usr/local/lib/python3.11/dist-packages (from fastembed>=0.2.2->llama-index-embeddings-fastembed) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (3.11.11)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (3.9.1)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (2.10.6)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (9.0.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (0.8.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.17.2)\n",
            "Collecting grpcio-tools>=1.41.0 (from qdrant-client>=1.7.1->llama-index-vector-stores-qdrant)\n",
            "  Downloading grpcio_tools-1.70.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant-client>=1.7.1->llama-index-vector-stores-qdrant)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.11/dist-packages (from qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (2.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file) (2025.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.18.3)\n",
            "Collecting protobuf<6.0dev,>=5.26.1 (from grpcio-tools>=1.41.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant)\n",
            "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from grpcio-tools>=1.41.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (75.1.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (0.14.0)\n",
            "Collecting h2<5,>=3 (from httpx[http2]>=0.20.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant)\n",
            "  Downloading h2-4.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed>=0.2.2->llama-index-embeddings-fastembed) (3.17.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed>=0.2.2->llama-index-embeddings-fastembed) (24.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (2024.11.6)\n",
            "Collecting coloredlogs (from onnxruntime!=1.20.0,>=1.17.0->fastembed>=0.2.2->llama-index-embeddings-fastembed)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed>=0.2.2->llama-index-embeddings-fastembed) (25.1.24)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed>=0.2.2->llama-index-embeddings-fastembed) (1.13.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.31->fastembed>=0.2.2->llama-index-embeddings-fastembed) (3.4.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (3.26.1)\n",
            "Collecting hyperframe<7,>=6.1 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant)\n",
            "  Downloading hyperframe-6.1.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting hpack<5,>=4.1 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant)\n",
            "  Downloading hpack-4.1.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime!=1.20.0,>=1.17.0->fastembed>=0.2.2->llama-index-embeddings-fastembed)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime!=1.20.0,>=1.17.0->fastembed>=0.2.2->llama-index-embeddings-fastembed) (1.3.0)\n",
            "Downloading llama_index_vector_stores_qdrant-0.4.3-py3-none-any.whl (11 kB)\n",
            "Downloading llama_index_embeddings_fastembed-0.3.0-py3-none-any.whl (2.7 kB)\n",
            "Downloading fastembed-0.5.1-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qdrant_client-1.13.2-py3-none-any.whl (306 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.6/306.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio_tools-1.70.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-4.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.20.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Downloading py_rust_stemmers-0.1.3-cp311-cp311-manylinux_2_28_x86_64.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.9/323.9 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-4.2.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-4.1.0-py3-none-any.whl (34 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hyperframe-6.1.0-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: py-rust-stemmers, mmh3, protobuf, portalocker, pillow, loguru, hyperframe, humanfriendly, hpack, h2, grpcio-tools, coloredlogs, onnxruntime, qdrant-client, fastembed, llama-index-vector-stores-qdrant, llama-index-embeddings-fastembed\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.6\n",
            "    Uninstalling protobuf-4.25.6:\n",
            "      Successfully uninstalled protobuf-4.25.6\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.1.0\n",
            "    Uninstalling pillow-11.1.0:\n",
            "      Successfully uninstalled pillow-11.1.0\n",
            "Successfully installed coloredlogs-15.0.1 fastembed-0.5.1 grpcio-tools-1.70.0 h2-4.2.0 hpack-4.1.0 humanfriendly-10.0 hyperframe-6.1.0 llama-index-embeddings-fastembed-0.3.0 llama-index-vector-stores-qdrant-0.4.3 loguru-0.7.3 mmh3-4.1.0 onnxruntime-1.20.1 pillow-10.4.0 portalocker-2.10.1 protobuf-5.29.3 py-rust-stemmers-0.1.3 qdrant-client-1.13.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "b67f82b070cc4c7d8bab210deb0144a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.3/622.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.7/412.7 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from llama_index.llms.groq import Groq\n",
        "from llama_index.core import Document\n",
        "import uuid\n",
        "from llama_index.core.text_splitter import SentenceSplitter\n",
        "import logging\n",
        "import sys\n",
        "import os\n",
        "import qdrant_client\n",
        "from IPython.display import Markdown, display\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.core import StorageContext\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
        "from llama_index.core import Settings\n"
      ],
      "metadata": {
        "id": "HMSL1vqxd3Hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up LLM"
      ],
      "metadata": {
        "id": "JHmfYX1BZLtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kx6BwlAdyBP",
        "outputId": "74dd2441-4fd7-45c8-9e3a-c8fc3e8fc34a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (0.18.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "aisXK2ECYp8g"
      },
      "outputs": [],
      "source": [
        "from groq import Groq\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "client = Groq(\n",
        "    api_key=userdata.get('groq_api'),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Generate a numbered, ordered list of technical topics I should learn if I want to work on LLM\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"deepseek-r1-distill-llama-70b\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnQG0swEcQmU",
        "outputId": "e435b2f4-dcfa-4854-e1bf-9ecc614ee4b1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<think>\n",
            "Okay, so I want to work on Large Language Models (LLMs), but I'm not really sure where to start. I've heard terms like transformers and neural networks thrown around, but I don't fully understand them. Maybe I should break it down into smaller parts.\n",
            "\n",
            "First, I think I need to get the basics of machine learning. I remember in school, we touched on supervised and unsupervised learning, but I'm fuzzy on the details. What's the difference again? Supervised is where you have labeled data, right? Like, you have inputs and outputs, and the model learns from that. Unsupervised is where you don't have labels, so the model finds patterns on its own. Then there's reinforcement learning, which I think is like training an agent to take actions to maximize some reward. I should probably review these concepts.\n",
            "\n",
            "Next, neural networks. I know they're inspired by the brain, with layers of nodes (neurons) connected by weights. There's the input layer, hidden layers, and the output layer. But I'm not clear on how backpropagation works. I think it's about adjusting the weights based on the error, but the math behind it is a bit hazy. I should look into that. Oh, and activation functions—like sigmoid, ReLU, tanh. What's the purpose of each?\n",
            "\n",
            "Deep learning frameworks like TensorFlow or PyTorch must be important. I've heard PyTorch is more flexible and easier for research. Maybe I should start with PyTorch. I need to learn how to build models, train them, and use pre-trained ones.\n",
            "\n",
            "Then, NLP fundamentals. I remember tokenization is breaking text into words or subwords. But what's the difference between word-level and subword tokenization? Word-level might miss rare words, while subword (like BPE or WordPiece) breaks them into smaller units. Part-of-speech tagging, named entity recognition—these are tasks I should understand. Also, language models—how do they predict the next word? I think n-gram models are traditional, but neural models use more complex architectures.\n",
            "\n",
            "Transformers are a big deal now. I know they're used in models like BERT and GPT. The paper \"Attention is All You Need\" introduced them. So, I need to understand self-attention, which allows the model to attend to all positions in the input simultaneously. But how does the attention mechanism work exactly? There are queries, keys, and values. Oh, and multi-head attention—so the model can focus on different aspects of the data.\n",
            "\n",
            "Pre-trained language models—I should learn how they're trained on large datasets to predict words. Fine-tuning is taking a pre-trained model and adapting it to a specific task. Transfer learning must be important here because it's about using knowledge from one task on another.\n",
            "\n",
            "Optimization techniques. I've heard of Adam optimizer, which is commonly used. But how does it differ from SGD? Maybe Adam combines the benefits of AdaGrad and RMSProp. Learning rate scheduling—like reducing the learning rate over time to help convergence. I should understand different strategies for that.\n",
            "\n",
            "The transformer architecture itself. The encoder-decoder structure, how each layer works with self-attention and feed-forward networks. Positional encoding because transformers don't have recurrence, so they need a way to encode position information.\n",
            "\n",
            "Advanced topics like attention mechanisms beyond the standard self-attention. Maybe there's sparse attention or other variants that are more efficient. Also, transformer variants like BERT, RoBERTa, GPT, T5. Each has its own approach—BERT uses masked language modeling, GPT is autoregressive, T5 does text-to-text.\n",
            "\n",
            "Efficiency and scaling. How to make models faster and handle more data. Techniques like pruning, quantization, knowledge distillation. I'm not sure how these work, but they seem important for deploying models.\n",
            "\n",
            "Specialized architectures for LLMs, such as using more layers or different attention patterns. Maybe some models use different normalization techniques or have modifications to improve performance on specific tasks.\n",
            "\n",
            "Evaluation metrics for LLMs. I know perplexity is a common one, but what does it really measure? Also, accuracy on specific tasks like GLUE benchmark. Human evaluation is mentioned—so sometimes, you just need people to assess the quality of generated text.\n",
            "\n",
            "Ethical considerations. I'm aware that models can have biases, so I need to understand how to identify and mitigate that. Privacy is another concern, especially with models potentially memorizing training data. Environmental impact because training big models uses a lot of resources. I should think about how to make training more efficient.\n",
            "\n",
            "Generative modeling techniques beyond basic language modeling. Like text generation, summarization, creative writing. How to control the output, maybe using prompts or other conditioning methods.\n",
            "\n",
            "Multimodal models that combine text with other data types like images or audio. I'm not sure how that integration works, but it's an interesting area.\n",
            "\n",
            "Reinforcement learning with LLMs. Using RL to fine-tune models based on rewards, maybe from human feedback. This could improve the model's behavior in specific tasks.\n",
            "\n",
            "Advanced optimization techniques. I've heard of things like layer-wise adaptive rate scaling (LARS) but don't know much about them. Also, distributed training methods to scale up model training across multiple GPUs or machines.\n",
            "\n",
            "Explainability and interpretability. How to understand what the model is doing, which is important for trust and debugging. Maybe techniques like attention visualization or feature importance.\n",
            "\n",
            "Low-resource and few-shot learning. How to make models work well with limited data. Few-shot learning might involve using prompts effectively.\n",
            "\n",
            "Continuous learning and adaptation. How models can update themselves with new data without forgetting old knowledge. This might be useful for models that need to stay current.\n",
            "\n",
            "Putting it all together, I should start with the basics, then move on to NLP and transformers, and gradually tackle the more advanced topics. Maybe find some courses or tutorials that cover these areas step by step. Practicing with projects would help solidify the concepts too.\n",
            "</think>\n",
            "\n",
            "To work on Large Language Models (LLMs), follow this organized learning path based on the thought process:\n",
            "\n",
            "1. **Machine Learning Basics**\n",
            "   - Study supervised, unsupervised, and reinforcement learning concepts.\n",
            "   - Understand the differences and applications of each paradigm.\n",
            "\n",
            "2. **Neural Networks Fundamentals**\n",
            "   - Learn about neural network architectures, including layers and nodes.\n",
            "   - Grasp backpropagation, including the mathematics behind weight adjustments.\n",
            "   - Explore activation functions like sigmoid, ReLU, and tanh.\n",
            "\n",
            "3. **Deep Learning Frameworks**\n",
            "   - Focus on PyTorch for its flexibility and research-friendly nature.\n",
            "   - Learn model building, training, and using pre-trained models.\n",
            "\n",
            "4. **NLP Fundamentals**\n",
            "   - Understand tokenization methods (word-level vs. subword).\n",
            "   - Study tasks like part-of-speech tagging and named entity recognition.\n",
            "   - Learn about language models, including n-gram and neural models.\n",
            "\n",
            "5. **Transformer Architecture**\n",
            "   - Read the \"Attention is All You Need\" paper.\n",
            "   - Understand self-attention, multi-head attention, and the transformer structure.\n",
            "\n",
            "6. **Pre-trained Language Models**\n",
            "   - Learn how models like BERT and GPT are trained.\n",
            "   - Understand fine-tuning and transfer learning in NLP.\n",
            "\n",
            "7. **Optimization Techniques**\n",
            "   - Study optimizers like Adam and learning rate scheduling strategies.\n",
            "\n",
            "8. **Advanced Transformer Topics**\n",
            "   - Explore attention variants and transformer models like BERT, RoBERTa, GPT, and T5.\n",
            "\n",
            "9. **Efficiency and Scaling**\n",
            "   - Investigate techniques like pruning, quantization, and knowledge distillation.\n",
            "\n",
            "10. **Specialized Architectures**\n",
            "    - Look into modifications for improved performance in specific tasks.\n",
            "\n",
            "11. **Evaluation Metrics**\n",
            "    - Understand perplexity, accuracy on benchmarks like GLUE, and human evaluation.\n",
            "\n",
            "12. **Ethical Considerations**\n",
            "    - Address bias mitigation, privacy concerns, and environmental impact.\n",
            "\n",
            "13. **Generative Modeling**\n",
            "    - Explore text generation, summarization, and creative writing techniques.\n",
            "\n",
            "14. **Multimodal Models**\n",
            "    - Study integration of text with images or audio.\n",
            "\n",
            "15. **Reinforcement Learning with LLMs**\n",
            "    - Learn about RL for fine-tuning models using rewards.\n",
            "\n",
            "16. **Advanced Optimization**\n",
            "    - Delve into techniques like LARS and distributed training.\n",
            "\n",
            "17. **Explainability and Interpretability**\n",
            "    - Use methods like attention visualization for model understanding.\n",
            "\n",
            "18. **Low-Resource and Few-Shot Learning**\n",
            "    - Learn to work with limited data and effective prompting.\n",
            "\n",
            "19. **Continuous Learning**\n",
            "    - Understand how models adapt with new data without forgetting.\n",
            "\n",
            "**Implementation Strategy:**\n",
            "- Start with basics, then progress to NLP and transformers.\n",
            "- Engage with courses, tutorials, and practical projects to reinforce learning.\n",
            "\n",
            "This structured approach will provide a comprehensive foundation for working on LLMs, from understanding core concepts to advanced techniques and ethical considerations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Hello! You are an assistant who will only reply in Roman urdu language\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Kia haal hai? Main aapki kaise madad kar sakta hoon?\"},\n",
        "        {\"role\": \"user\", \"content\": \"Create a well-organized routine for me\"}\n",
        "    ],\n",
        "    model=\"deepseek-r1-distill-llama-70b\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O62FXc8OcdDh",
        "outputId": "7e3c8d5e-5433-43b8-a00a-592b4b8faeed"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<think>\n",
            "Alright, the user just asked me to create a well-organized routine for them. They mentioned they want it in Roman Urdu, so I need to make sure I structure it clearly in that format. \n",
            "\n",
            "First, I should consider the key components of a daily routine. Usually, it starts from waking up, then exercise, breakfast, work or study, lunch, some leisure time, more work, evening activities, dinner, and then winding down before bed. \n",
            "\n",
            "I need to break down each part of the day into manageable chunks. Maybe allocate specific time slots so the routine is structured. Let's see, waking up at 6 AM is a good start, then exercise until 7. After that, breakfast and getting ready. \n",
            "\n",
            "For work or study, I can set aside a solid block in the morning. Lunch at 1 PM, then some rest. The afternoon can be for more work or studies. Leisure time in the evening, maybe around 5 PM. Dinner at 8 PM and some relaxation before bed at 10:30 PM. \n",
            "\n",
            "I should also include some tips to make the routine effective. Maybe advice on sticking to the schedule, taking short breaks, staying hydrated, eating healthy, exercising regularly, and getting enough sleep. \n",
            "\n",
            "I need to present all this in Roman Urdu, making sure it's easy to read and follow. Each time slot should be clear, and the tips should be concise but helpful. \n",
            "\n",
            "Let me organize this into sections: Morning, Day, Evening, Night, and then the tips. That way, it's easy for the user to follow. I'll use bullet points or numbers where necessary to enhance readability. \n",
            "\n",
            "I should also make sure the language is simple and straightforward, avoiding any complicated words since the user might not be highly fluent in English. \n",
            "\n",
            "Double-checking the time allocations to ensure they are realistic and allow for a balanced day. It's important the routine isn't too rigid but still provides structure. \n",
            "\n",
            "Including elements like leisure time and relaxation is crucial to maintain a healthy work-life balance. Maybe suggest activities like reading or walking in the evening. \n",
            "\n",
            "Alright, putting it all together now, ensuring each part flows logically and covers all aspects of the day. Making sure the Roman Urdu is accurate and easy to understand. \n",
            "\n",
            "I think that's a solid plan. Time to draft the response accordingly.\n",
            "</think>\n",
            "\n",
            "Yeh raha aapka well-organized daily routine (Roman Urdu mein):\n",
            "\n",
            "---\n",
            "\n",
            "### **Morning Routine (Subah Ki Routine):**\n",
            "1. **6:00 AM - Wake Up (Jagna):**\n",
            "   - Bed se uthkar duaa karein.\n",
            "   - Morning walk ya exercise karein (30 minutes).\n",
            "\n",
            "2. **6:30 AM - Fresh Up (Tayyar Hona):**\n",
            "   - Brush karein, face wash karein, aur naha dhokar fresh ho jayein.\n",
            "\n",
            "3. **7:00 AM - Breakfast (Nashta):**\n",
            "   - Healthy breakfast karein (jaise poha, omelette, ya phal).\n",
            "\n",
            "4. **7:30 AM - Get Ready (Tayyar Hona):**\n",
            "   - Office ya studies ke liye tayyar ho jayein.\n",
            "\n",
            "---\n",
            "\n",
            "### **Day Routine (Din Ki Routine):**\n",
            "5. **8:00 AM - Start Work/Study (Kaam ya Padhai Shuru Karein):**\n",
            "   - Apne goals ko achieve karne ke liye focused rahein.\n",
            "   - Short breaks lein (5-10 minutes) har 1 ghante ke baad.\n",
            "\n",
            "6. **1:00 PM - Lunch (Dopahar Ka Khana):**\n",
            "   - Healthy aur poora khana karein.\n",
            "   - 10-15 minutes ka rest lein.\n",
            "\n",
            "7. **1:30 PM - Resume Work/Study (Kaam ya Padhai Dobara Shuru Karein):**\n",
            "   - Agla session focused aur productive banayein.\n",
            "\n",
            "8. **5:00 PM - Evening Break (Sham Ki Chai):**\n",
            "   - Chai ya coffee karein, aur thoda samay relax karein.\n",
            "\n",
            "---\n",
            "\n",
            "### **Evening Routine (Sham Ki Routine):**\n",
            "9. **5:30 PM - Leisure Time (Aaram Karne Ka Samay):**\n",
            "   - Kuchh samay apne shauk ke liye nikaalein (jaise kitaab padhna, gaana sunna, ya walk karna).\n",
            "\n",
            "10. **6:30 PM - Exercise/Yoga (Vyaayam ya Yoga):**\n",
            "    - Rozana 30-45 minutes exercise karein.\n",
            "\n",
            "11. **7:30 PM - Dinner (Shaam Ka Khana):**\n",
            "    - Healthy aur halka khana karein.\n",
            "\n",
            "---\n",
            "\n",
            "### **Night Routine (Raat Ki Routine):**\n",
            "12. **8:00 PM - Relaxation Time (Aaram Karne Ka Samay):**\n",
            "    - Family ya doston ke saath samay bitayein.\n",
            "    - TV dekhna ya koi hobby pursue karein.\n",
            "\n",
            "13. **9:30 PM - Wind Down (Din Bhar Ka End):**\n",
            "    - Apne din ki planning aur goals review karein.\n",
            "    - Kal ke liye plan banayein.\n",
            "\n",
            "14. **10:00 PM - Prepare for Bed (Bistar Tyaar Karein):**\n",
            "    - Light reading ya meditation karein.\n",
            "\n",
            "15. **10:30 PM - Sleep (Sone Jaayein):**\n",
            "    - Puray 7-8 ghante ki neend lein.\n",
            "\n",
            "---\n",
            "\n",
            "### **Tips for a Successful Routine:**\n",
            "- Har din is routine ka strictly paalan karein.\n",
            "- Short breaks lete rahein taaki dimaag fresh rahe.\n",
            "- Paani peena aur healthy khana khayein.\n",
            "- Exercise ko apni routine ka hissa banayein.\n",
            "- Neend ka pura dhyan rakhein.\n",
            "\n",
            "Umeed hai yeh routine aapko organized aur productive banayegi! 😊\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Max Token"
      ],
      "metadata": {
        "id": "rjdNaqM_i8Pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Generate a numbered, ordered list of technical topics I should learn if I want to work on LLM\",\n",
        "        }\n",
        "    ],\n",
        "    max_tokens=100,\n",
        "    model=\"deepseek-r1-distill-llama-70b\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtTwFpy9i66T",
        "outputId": "2f707214-eeeb-42a4-b803-fe0bb1dace7c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<think>\n",
            "Okay, so I want to work on Large Language Models (LLMs), but I'm not really sure where to start. I know that LLMs are a big part of AI these days, but the field seems really vast. Let me try to break this down.\n",
            "\n",
            "First, I remember that LLMs are built using machine learning, so I probably need to get a solid foundation in that. But wait, machine learning itself has a lot of areas. There's supervised learning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion.choices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJHqwPlqflVJ",
        "outputId": "f287da2b-4647-4ca2-8add-32d3db601b89"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content=\"<think>\\nOkay, so I want to work on Large Language Models (LLMs), but I'm not really sure where to start. I know that LLMs are a big part of AI these days, but the field seems really vast. Let me try to break this down.\\n\\nFirst, I remember that LLMs are built using machine learning, so I probably need to get a solid foundation in that. But wait, machine learning itself has a lot of areas. There's supervised learning,\", role='assistant', function_call=None, reasoning=None, tool_calls=None))]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Temperature\n",
        "Higher temperature means more creativity"
      ],
      "metadata": {
        "id": "kSsrjha-jPmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Generate a numbered, ordered list of technical topics I should learn if I want to work on LLM\",\n",
        "        }\n",
        "    ],\n",
        "    temperature=0.7,\n",
        "    model=\"deepseek-r1-distill-llama-70b\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtD18EiqjF8F",
        "outputId": "78a971fb-7912-46d8-b962-b28e34f481fa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<think>\n",
            "Okay, so I want to work on Large Language Models (LLMs), but I'm not exactly sure where to start. I've heard a lot about machine learning and AI, but the specifics are a bit overwhelming. Let me try to break this down step by step.\n",
            "\n",
            "First, I know that LLMs are a type of AI, so I probably need to understand the basics of machine learning. I remember hearing about supervised and unsupervised learning. Supervised learning is where the model is trained on labeled data, right? Like, each example has an input and the correct output. Unsupervised learning is where the model finds patterns in unlabeled data. But I'm not entirely clear on how that applies to language models. Maybe in NLP, unsupervised learning is used for things like clustering texts or finding topics?\n",
            "\n",
            "Then there's deep learning. I think that's a subset of machine learning that uses neural networks with many layers. Neural networks are inspired by the human brain, with nodes (neurons) connected in layers. I should probably learn about different types of neural networks, like CNNs and RNNs. Wait, CNNs are Convolutional Neural Networks, mainly used for images, right? But RNNs are Recurrent Neural Networks, which are good for sequences like text or time series data. Oh, and LSTMs are a type of RNN that helps with long-term dependencies, which is useful for longer texts.\n",
            "\n",
            "Transformer architectures are a big deal in LLMs. I've heard of BERT, RoBERTa, and GPT models. Transformers use self-attention mechanisms, which allow the model to weigh the importance of different words in a sentence. I should definitely dive deeper into how transformers work, maybe starting with the original Transformer paper.\n",
            "\n",
            "Moving on to NLP fundamentals. I know tokenization is breaking text into words or subwords. But what's the difference between tokens and subwords? I think subwords are smaller units, like parts of words, to handle rare or unseen words better. Part-of-speech tagging is identifying nouns, verbs, etc., but I'm not sure how that's used in LLMs. Maybe for feature extraction or understanding sentence structure.\n",
            "\n",
            "Named Entity Recognition (NER) is about identifying entities like names and places. Dependency parsing is about sentence structure, like subject-verb-object relationships. Coreference resolution is figuring out what pronouns refer to, which is important for understanding context in longer texts.\n",
            "\n",
            "Pretrained models are a cornerstone of modern NLP. Models like BERT are pretrained on large datasets to learn language representations. Fine-tuning is taking these models and adapting them to specific tasks, like sentiment analysis or question answering. I think transfer learning is important here because it allows using knowledge from one task on another, which is efficient.\n",
            "\n",
            "Language model architecture involves understanding how the model is structured. I need to know about encoder-decoder models, like the original Transformer, where the encoder processes input and the decoder generates output. Training objectives are like the goals the model is trained on. Masked language modeling is where some words are hidden and the model predicts them, which is how BERT is trained. Next sentence prediction is predicting if two sentences are adjacent, which helps in understanding relationships between sentences.\n",
            "\n",
            "Optimization techniques are crucial for training these large models. Adam and AdamW are optimization algorithms that adapt learning rates for each parameter. Learning rate scheduling is adjusting the learning rate during training to improve convergence. Gradient clipping is limiting gradient values to prevent exploding gradients, which can destabilize training.\n",
            "\n",
            "Regularization techniques prevent overfitting. Dropout randomly deactivates neurons during training to stop the model from relying too much on any single neuron. Weight decay adds a penalty term to the loss to discourage large weights. Layer normalization normalizes the activations of each layer to stabilize training.\n",
            "\n",
            "Advanced topics include attention mechanisms beyond the standard self-attention. Maybe there are variations like sparse attention or other efficiency improvements. Model parallelism is about splitting the model across multiple GPUs or machines because LLMs are huge. Pipelining is processing different parts of the model in stages to improve computation efficiency.\n",
            "\n",
            "Efficient inference is important for deploying models. Quantization reduces the precision of model weights to make the model smaller and faster. Pruning removes unnecessary weights or neurons to reduce model size. Knowledge distillation is teaching a smaller model (student) to mimic a larger model (teacher) to maintain performance while reducing size.\n",
            "\n",
            "Multilingual models are trained on multiple languages, so they can understand and generate text in different languages. Few-shot and zero-shot learning are about the model's ability to perform tasks with few or no examples, which is pretty cool.\n",
            "\n",
            "Specialized models include multimodal models that handle text and images together, like CLIP or Flamingo. Instruction-tuned models are optimized to follow instructions, like GPT-3.5 or PaLM, which can perform specific tasks better.\n",
            "\n",
            "For deployment, I need to know about model serving, which is making the model available as an API. Rate limiting is controlling how many requests are handled to prevent abuse or overuse. Monitoring is tracking the model's performance and usage in real-time. A/B testing is comparing different model versions to see which performs better.\n",
            "\n",
            "Ethics, fairness, and safety are important to ensure models don't perpetuate biases or harm users. I need to understand bias mitigation strategies and how to make models more transparent and explainable. Safety measures prevent the model from generating harmful content.\n",
            "\n",
            "Staying updated is crucial in this fast-moving field. Following research papers, joining communities, and participating in competitions can help me keep up with the latest developments.\n",
            "\n",
            "The tools and frameworks are a bit intimidating. TensorFlow and PyTorch are the main deep learning frameworks. PyTorch is more flexible, while TensorFlow has more production tools. Hugging Face Transformers is a popular library for NLP. ONNX is for model conversion and optimization. Weights & Biases and TensorBoard are for tracking experiments and visualizing training metrics.\n",
            "\n",
            "Cloud platforms like AWS, GCP, and Azure provide infrastructure for training and deploying models. Docker and Kubernetes are for containerization and orchestration, which help in managing deployments.\n",
            "\n",
            "Math and theory are the foundation. Linear algebra is about vectors and matrices, which are essential for neural networks. Calculus is needed for optimization and understanding how models learn. Probability and statistics are crucial for understanding distributions and uncertainties in data. Information theory explains concepts like entropy and cross-entropy, which are used in loss functions.\n",
            "\n",
            "The suggested learning path makes sense: start with ML basics, then NLP and deep learning, move to advanced topics, and finally deployment and ethics. It's a logical progression from foundation to application.\n",
            "\n",
            "I think I need to prioritize these topics, maybe start with the basics of ML and NLP, then move into deep learning and transformers. As I get comfortable, I can dive into more advanced areas like model parallelism and efficient inference. Deployment and ethics should be considered from the start to ensure responsible AI practices.\n",
            "\n",
            "I might be missing some areas or overcomplicating others, but this seems like a solid starting point. I should create a study plan, maybe using online courses and hands-on projects to apply what I learn. Practicing with real-world datasets and contributing to open-source projects could also provide valuable experience.\n",
            "</think>\n",
            "\n",
            "To work on Large Language Models (LLMs), follow this organized and prioritized learning path:\n",
            "\n",
            "### 1. **Foundations of Machine Learning**\n",
            "   - **Supervised & Unsupervised Learning**: Understand the basics, including applications in NLP.\n",
            "   - **Deep Learning**: Study neural networks, focusing on CNNs, RNNs, and LSTMs.\n",
            "\n",
            "### 2. **Transformer Architecture**\n",
            "   - **Transformers & Self-Attention**: Delve into the original Transformer paper and mechanisms like self-attention.\n",
            "   - **BERT, RoBERTa, GPT**: Explore these models and their architectures.\n",
            "\n",
            "### 3. **NLP Fundamentals**\n",
            "   - **Tokenization & Subwords**: Learn about breaking text into tokens and subwords.\n",
            "   - **Part-of-Speech Tagging, NER, Dependency Parsing, Coreference Resolution**: Understand these tasks and their roles in NLP.\n",
            "\n",
            "### 4. **Pretrained Models & Transfer Learning**\n",
            "   - **Pretraining & Fine-tuning**: Use models like BERT and apply transfer learning for specific tasks.\n",
            "\n",
            "### 5. **Language Model Architecture & Training**\n",
            "   - **Encoder-Decoder Models**: Study the Transformer architecture.\n",
            "   - **Training Objectives**: Explore masked language modeling and next sentence prediction.\n",
            "\n",
            "### 6. **Optimization & Regularization**\n",
            "   - **Optimization Techniques**: Learn Adam, AdamW, learning rate scheduling, and gradient clipping.\n",
            "   - **Regularization**: Understand dropout, weight decay, and layer normalization.\n",
            "\n",
            "### 7. **Advanced Topics**\n",
            "   - **Attention Mechanisms**: Explore beyond standard self-attention.\n",
            "   - **Model Parallelism & Pipelining**: Learn to split models across GPUs.\n",
            "   - **Efficient Inference**: Study quantization, pruning, and knowledge distillation.\n",
            "\n",
            "### 8. **Multilingual, Few-shot, Zero-shot Learning**\n",
            "   - **Multilingual Models**: Understand models trained on multiple languages.\n",
            "   - **Few-shot & Zero-shot Learning**: Learn task performance with minimal examples.\n",
            "\n",
            "### 9. **Specialized Models**\n",
            "   - **Multimodal Models**: Study models like CLIP for text-image tasks.\n",
            "   - **Instruction-Tuned Models**: Explore models optimized for instructions.\n",
            "\n",
            "### 10. **Deployment & Monitoring**\n",
            "   - **Model Serving & API**: Deploy models and manage requests.\n",
            "   - **Rate Limiting & Monitoring**: Control and track model usage and performance.\n",
            "   - **A/B Testing**: Compare model versions for effectiveness.\n",
            "\n",
            "### 11. **Ethics, Fairness, Safety**\n",
            "   - **Bias Mitigation & Transparency**: Ensure models are fair and explainable.\n",
            "   - **Safety Measures**: Prevent harmful content generation.\n",
            "\n",
            "### 12. **Stay Updated**\n",
            "   - **Research & Community**: Follow papers, join communities, and participate in competitions.\n",
            "\n",
            "### 13. **Tools & Frameworks**\n",
            "   - **TensorFlow, PyTorch, Hugging Face**: Use these for deep learning and NLP.\n",
            "   - **ONNX, Docker, Kubernetes**: Optimize and manage model deployments.\n",
            "   - **Cloud Platforms**: Utilize AWS, GCP, Azure for infrastructure.\n",
            "\n",
            "### 14. **Math & Theory**\n",
            "   - **Linear Algebra, Calculus, Probability, Statistics, Information Theory**: Build a strong foundation.\n",
            "\n",
            "### Suggested Learning Path\n",
            "1. **Start with ML Basics**: Cover supervised/unsupervised learning and deep learning.\n",
            "2. **Dive into NLP & Transformers**: Explore fundamentals and architectures.\n",
            "3. **Advanced Topics**: Tackle parallelism, efficient inference, and specialized models.\n",
            "4. **Deployment & Ethics**: Ensure responsible AI practices from the start.\n",
            "\n",
            "### Action Plan\n",
            "- **Study Plan**: Use online courses and hands-on projects.\n",
            "- **Practice**: Work with real-world datasets and contribute to open-source projects.\n",
            "\n",
            "This structured approach will help you build a strong foundation and progress effectively in working with LLMs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Vision model"
      ],
      "metadata": {
        "id": "TGPr-eWA0gcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "import base64\n",
        "\n",
        "\n",
        "# Function to encode the image\n",
        "def encode_image(image_path):\n",
        "  with open(image_path, \"rb\") as image_file:\n",
        "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "# Path to your image\n",
        "image_path = \"sf.png\"\n",
        "\n",
        "# Getting the base64 string\n",
        "base64_image = encode_image(image_path)\n",
        "\n",
        "client = Groq()\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\n",
        "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
        "                    },\n",
        "                },\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama-3.2-90b-vision-preview\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tt8mUBDkoyXs",
        "outputId": "b172a461-70b2-4cab-a2fe-bfd9fd59136a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The image depicts a table with various food containers, likely from a restaurant or catering service. The containers are arranged in two stacks: one stack of rectangular containers with clear lids and another stack of smaller, square containers with white lids. In the background, there are several black chairs and a table setting with utensils.\n",
            "\n",
            "Here is a detailed description of the items in the image:\n",
            "\n",
            "*   **Rectangular Containers**\n",
            "    *   Clear lids\n",
            "    *   Stacked on top of each other\n",
            "    *   Contain food\n",
            "*   **Square Containers**\n",
            "    *   White lids\n",
            "    *   Smaller than the rectangular containers\n",
            "    *   Also contain food\n",
            "*   **Chairs**\n",
            "    *   Black\n",
            "    *   Arranged around the table\n",
            "*   **Table Setting**\n",
            "    *   Utensils (forks, knives, spoons)\n",
            "    *   Plates (not visible)\n",
            "\n",
            "Overall, the image suggests that the food is prepared for takeout or delivery, as evidenced by the use of plastic containers. The presence of utensils and plates in the background implies that the food is intended to be eaten immediately, rather than stored for later consumption.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your image\n",
        "image_path = \"plant.png\"\n",
        "\n",
        "# Getting the base64 string\n",
        "base64_image = encode_image(image_path)\n",
        "\n",
        "client = Groq()\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": \"What species is this?\"},\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\n",
        "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
        "                    },\n",
        "                },\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama-3.2-90b-vision-preview\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyR-OPtx3BdD",
        "outputId": "dd5ffc7f-cfe4-4885-b782-9877fb320fc3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A carnivorous plant belonging to the family Nepenthaceae, native to tropical regions of Asia, including Malaysia, Indonesia, and the Philippines.\n",
            "\n",
            "**Characteristics:**\n",
            "\n",
            "• Modified leaves that form a deep, slippery cup to trap and digest insects and other small animals\n",
            "• Carnivorous behavior utilizes complex mechanisms to capture and digest prey\n",
            "• Striking appearance, with a range of colors and patterns on the pitchers\n",
            "\n",
            "**Habitat:**\n",
            "\n",
            "• Thrives in high-humidity environments with abundant sunlight\n",
            "• Often found in:\n",
            "  • Tropical forests\n",
            "  • Swamps\n",
            "  • Mountainous regions\n",
            "\n",
            "**Importance:**\n",
            "\n",
            "• Unique, fascinating plant with a specialized feeding mechanism\n",
            "• Widely collected and cultivated by enthusiasts around the world\n",
            "• Plays a vital role in tropical ecosystems, providing nutrition to animals in nutrient-poor environments\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_image(image_path):\n",
        "  with open(image_path, \"rb\") as image_file:\n",
        "    image_url=base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "    return {\n",
        "              \"type\": \"image_url\",\n",
        "              \"image_url\": {\n",
        "              \"url\": f\"data:image/jpeg;base64,{image_url}\",\n",
        "              },\n",
        "            }"
      ],
      "metadata": {
        "id": "of1W954H58fH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your image\n",
        "image_path = \"invoice.png\"\n",
        "\n",
        "# Getting the base64 string\n",
        "base64_image = encode_image(image_path)\n",
        "\n",
        "client = Groq()\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\":\n",
        "                 \"\"\"\n",
        "                Generate a JSON object representing the contents\n",
        "                of this invoice.  It should include all dates,\n",
        "                dollar amounts, and addresses.\n",
        "                Only respond with the JSON itself.\n",
        "                 \"\"\"\n",
        "\n",
        "                 },\n",
        "                encode_image(image_path),\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama-3.2-90b-vision-preview\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldjzRNPz34vT",
        "outputId": "56b81f73-477d-4b6e-fa4c-17d81cf1700a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```\n",
            "{\n",
            "  \"invoice_number\": \"INV-2024-0042\",\n",
            "  \"invoice_date\": \"March 17, 2025\",\n",
            "  \"due_date\": \"April 16, 2025\",\n",
            "  \"bill_to\": {\n",
            "    \"name\": \"ACME Corporation\",\n",
            "    \"address\": {\n",
            "      \"street\": \"789 Market Street, Suite 500\",\n",
            "      \"city\": \"Los Angeles\",\n",
            "      \"state\": \"CA\",\n",
            "      \"zip\": \"90015\"\n",
            "    }\n",
            "  },\n",
            "  \"description\": [\n",
            "    {\n",
            "      \"quantity\": 1,\n",
            "      \"unit_price\": 5000.00,\n",
            "      \"amount\": 5000.00,\n",
            "      \"description\": \"Enterprise Software License\"\n",
            "    },\n",
            "    {\n",
            "      \"quantity\": 40,\n",
            "      \"unit_price\": 150.00,\n",
            "      \"amount\": 6000.00,\n",
            "      \"description\": \"Implementation Services\"\n",
            "    },\n",
            "    {\n",
            "      \"quantity\": 1,\n",
            "      \"unit_price\": 2500.00,\n",
            "      \"amount\": 2500.00,\n",
            "      \"description\": \"Premium Support Plan (Annual)\"\n",
            "    }\n",
            "  ],\n",
            "  \"subtotal\": 13500.00,\n",
            "  \"tax\": 1147.50,\n",
            "  \"total\": 14647.50\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"How does open source companies work\"\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama-3.2-90b-vision-preview\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRv4lwpN1BrC",
        "outputId": "6d1d260c-5bcc-469c-864e-99fbf3e47cf4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Open-source companies are organizations that develop and maintain software using the open-source model, which allows users to access, modify, and distribute the software for free. Here's a general overview of how open-source companies work:\n",
            "\n",
            "**Key characteristics:**\n",
            "\n",
            "1. **Free software**: Open-source companies provide their software for free, or at a minimal cost.\n",
            "2. **Open-source code**: The source code of the software is made available to the public, allowing users to review, modify, and distribute it.\n",
            "3. **Community-driven**: Open-source companies often rely on a community of users, developers, and contributors to help maintain and improve the software.\n",
            "\n",
            "**Revenue streams:**\n",
            "\n",
            "1. **Support and services**: Many open-source companies offer support, consulting, and training services to users, generating revenue from these services.\n",
            "2. **Licensing fees**: Some open-source companies charge fees for commercial use of their software, while still making the software available for free to individuals and non-profit organizations.\n",
            "3. **Donations**: Some open-source companies rely on donations to fund their development and maintenance efforts.\n",
            "4. **Sponsored features**: Some open-source companies offer sponsored features or plugins, which are developed and maintained by the company for specific clients or partners.\n",
            "5. **Platform and hosting**: Some open-source companies generate revenue by offering hosting services for their software or related platforms.\n",
            "\n",
            "**Models for open-source companies:**\n",
            "\n",
            "1. **Dual-licensing model**: Companies offer their software under an open-source license, while also providing a commercial license for proprietary use.\n",
            "2. **Service-based model**: Companies provide support, consulting, and training services to users, while also offering their software for free.\n",
            "3. **Foundation-based model**: Companies establish a non-profit foundation to manage and maintain the open-source software, while generating revenue through support and services.\n",
            "4. **Hybrid model**: Companies combine elements of the dual-licensing, service-based, and foundation-based models to generate revenue.\n",
            "\n",
            "**Examples of open-source companies:**\n",
            "\n",
            "1. **Red Hat**: Develops and supports open-source software, including Linux, and generates revenue through support and services.\n",
            "2. **Canonical**: Develops and supports the open-source Linux distribution Ubuntu, and generates revenue through support and services.\n",
            "3. **Appium**: Develops and maintains the open-source mobile app testing framework Appium, and generates revenue through support and training services.\n",
            "4. **Odoo**: Develops and maintains the open-source enterprise resource planning (ERP) software Odoo, and generates revenue through consulting, support, and hosting services.\n",
            "\n",
            "**Benefits and challenges:**\n",
            "\n",
            "**Benefits:**\n",
            "\n",
            "1. **Community engagement**: Open-source companies can tap into a community of developers and users, driving innovation and collaboration.\n",
            "2. **Rapid development**: Open-source companies can accelerate development by leveraging a community of contributors.\n",
            "3. **Increased adoption**: Open-source companies can reach a wider audience, as users are more likely to adopt free and open software.\n",
            "\n",
            "**Challenges:**\n",
            "\n",
            "1. **Revenue generation**: Open-source companies must find innovative ways to generate revenue, as traditional licensing fees are not applicable.\n",
            "2. **Maintenance and support**: Open-source companies must invest in maintenance and support to ensure the quality and reliability of their software.\n",
            "3. **Intellectual property**: Open-source companies must carefully manage intellectual property rights to ensure the integrity of their software.\n",
            "\n",
            "In summary, open-source companies work by providing free software, relying on community engagement, and generating revenue through support and services, licensing fees, donations, and platform and hosting services. They use various models, such as the dual-licensing model, service-based model, foundation-based model, and hybrid model, to achieve this.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using stream which will speed up the call to first token"
      ],
      "metadata": {
        "id": "ozkYic2W74_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"How does open source companies work\"\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama-3.2-90b-vision-preview\",\n",
        "    stream=True,\n",
        ")\n",
        "\n",
        "for chunk in chat_completion:\n",
        "    if chunk.choices and chunk.choices[0].delta.content:\n",
        "        print(chunk.choices[0].delta.content, end=\"\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWblRkDY1x_i",
        "outputId": "dbc36588-625f-467d-9cd7-f90531b63699"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Open-source companies operate on a unique business model that combines free and open-source software (FOSS) principles with commercial goals. Here's an overview of how they work:\n",
            "\n",
            "**Key characteristics:**\n",
            "\n",
            "1. **Free and open-source software**: Open-source companies develop and release software under an open-source license, which allows users to use, modify, and distribute the software freely.\n",
            "2. **Community-driven**: Open-source companies often rely on a community of volunteer developers, contributors, and users to contribute to the software's development, testing, and maintenance.\n",
            "3. **Transparent development**: Open-source companies typically use open-source collaboration tools and platforms, such as GitHub, to manage their development process, making their codebase and development history publicly visible.\n",
            "4. **Commercial support**: While the software is free, open-source companies offer commercial support, services, and features to generate revenue.\n",
            "\n",
            "**Revenue models:**\n",
            "\n",
            "1. **Support and services**: Open-source companies offer various levels of support, such as bug fixing, troubleshooting, and consulting services, for a fee.\n",
            "2. **Licensing**: Some open-source companies offer proprietary licenses to large enterprises, which includes additional features, support, or customization.\n",
            "3. **Hosting and subscriptions**: Some open-source companies offer hosting services or subscription-based models for access to their software, support, and updates.\n",
            "4. **Training and education**: Open-source companies can offer training, workshops, or online courses to educate users about their software and generate revenue.\n",
            "5. **Donations**: Some open-source companies rely on donations from their user community to support their development efforts.\n",
            "\n",
            "**Examples:**\n",
            "\n",
            "1. **Red Hat**: A leading open-source software company that offers Linux operating systems, middleware, and storage solutions, with a focus on enterprise customers.\n",
            "2. **Canonical (Ubuntu)**: A UK-based company behind the popular Ubuntu Linux operating system, which generates revenue through support services, consulting, and training.\n",
            "3. **MySQL**: An open-source relational database management system (RDBMS) company that was acquired by Oracle, but continues to offer support and services.\n",
            "4. **Apache Software Foundation**: A non-profit organization that develops and maintains various open-source projects, including the Apache HTTP Server, with a reliance on donations.\n",
            "\n",
            "**Benefits:**\n",
            "\n",
            "1. **Community involvement**: Open-source companies promote collaboration and community engagement, fostering innovation and faster development cycles.\n",
            "2. **Cost-effective**: Users can access high-quality software without the licensing costs associated with proprietary software.\n",
            "3. **Flexibility**: Open-source software can be customized to meet specific needs and use cases.\n",
            "4. **Security**: Open-source software can benefit from the scrutiny of a large community, reducing the risk of vulnerabilities.\n",
            "\n",
            "**Challenges:**\n",
            "\n",
            "1. **Revenue generation**: Balancing revenue needs with the principles of open-source software can be challenging.\n",
            "2. **Quality control**: Ensuring the quality and reliability of community-driven software can be difficult.\n",
            "3. **Maintaining a balance**: Walking the line between open-source principles and commercial interests can be a delicate balance.\n",
            "\n",
            "Overall, open-source companies have disrupted traditional software business models by providing high-quality software at no upfront cost. While they face challenges, these companies have proven that it's possible to build profitable businesses around open-source software."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Enterprice prompt"
      ],
      "metadata": {
        "id": "jpfZdFItGe5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enterprise_prompt = \"\"\"\n",
        "Analyze the following customer service call transcript and generate a JSON summary of the interaction:\n",
        "\n",
        "<transcript>\n",
        "[INSERT CALL TRANSCRIPT HERE]\n",
        "</transcript>\n",
        "\n",
        "Instructions:\n",
        "<instructions>\n",
        "1. Read the transcript carefully.\n",
        "2. Analyze the transcript, focusing on the main issue, resolution, and any follow-up required.\n",
        "3. Generate a JSON object summarizing the key aspects of the interaction according to the specified structure.\n",
        "\n",
        "Important guidelines:\n",
        "- Confidentiality: Omit all specific customer data like names, phone numbers, and email addresses.\n",
        "- Character limit: Restrict each text field to a maximum of 100 characters.\n",
        "- Maintain a professional tone in your summary.\n",
        "\n",
        "Output format:\n",
        "Generate a JSON object with the following structure:\n",
        "<json>\n",
        "{\n",
        "  \"summary\": {\n",
        "    \"customerIssue\": \"Brief description of the main problem or reason for the call\",\n",
        "    \"resolution\": \"How the issue was addressed or resolved, if applicable\",\n",
        "    \"followUpRequired\": true/false,\n",
        "    \"followUpDetails\": \"Description of any necessary follow-up actions, or null if none required\"\n",
        "  },\n",
        "  \"status\": \"COMPLETE\",\n",
        "  \"ambiguities\": [\"List of any unclear or vague points in the conversation, or an empty array if none\"]\n",
        "}\n",
        "</json>\n",
        "\n",
        "Insufficient data criteria:\n",
        "   If any of these conditions are met:\n",
        "   a) The transcript has fewer than 5 total exchanges\n",
        "   b) The customer's issue is unclear\n",
        "   c) The call is garbled, incomplete, or is hindered by a language barrier\n",
        "   Then return ONLY the following JSON:\n",
        "   {\n",
        "     \"status\": \"INSUFFICIENT_DATA\"\n",
        "   }\n",
        "\n",
        "Examples:\n",
        "<examples>\n",
        "1. Complete interaction:\n",
        "<transcript>\n",
        "Agent: Thank you for calling Acme Smart Home Support. This is Alex. How may I assist you today?\n",
        "Customer: Hi Alex, my Acme SmartTherm isn't maintaining the temperature I set. It's set to 72 but the house is much warmer.\n",
        "Agent: I'm sorry to hear that. Let's troubleshoot. Is your SmartTherm connected to Wi-Fi?\n",
        "Customer: Yes, the Wi-Fi symbol is showing on the display.\n",
        "Agent: Great. Let's recalibrate your SmartTherm. Press and hold the menu button for 5 seconds.\n",
        "Customer: Okay, done. A new menu came up.\n",
        "Agent: Perfect. Navigate to \"Calibration\" and press select. Adjust the temperature to match your room thermometer.\n",
        "Customer: Alright, I've set it to 79 degrees to match.\n",
        "Agent: Great. Press select to confirm. It will recalibrate, which may take a few minutes. Check back in an hour to see if it's fixed.\n",
        "Customer: Okay, I'll do that. Thank you for your help, Alex.\n",
        "Agent: You're welcome! Is there anything else I can assist you with today?\n",
        "Customer: No, that's all. Thanks again.\n",
        "Agent: Thank you for choosing Acme Smart Home. Have a great day!\n",
        "</transcript>\n",
        "\n",
        "<thinking>\n",
        "Main issue: SmartTherm not maintaining set temperature\n",
        "Resolution: Guided customer through recalibration process\n",
        "Follow-up: Not required, but customer should check effectiveness after an hour\n",
        "Ambiguities: None identified\n",
        "</thinking>\n",
        "\n",
        "<json>\n",
        "{\n",
        "  \"summary\": {\n",
        "    \"customerIssue\": \"SmartTherm not maintaining set temperature, showing higher than set 72 degrees\",\n",
        "    \"resolution\": \"Guided customer through SmartTherm recalibration process\",\n",
        "    \"followUpRequired\": false,\n",
        "    \"followUpDetails\": null\n",
        "  },\n",
        "  \"status\": \"COMPLETE\",\n",
        "  \"ambiguities\": []\n",
        "}\n",
        "</json>\n",
        "\n",
        "2. Interaction requiring follow-up:\n",
        "<transcript>\n",
        "Agent: Acme Smart Home Support, this is Jamie. How can I help you?\n",
        "Customer: Hi, I just installed my new Acme SmartCam, but I can't get it to connect to my Wi-Fi.\n",
        "Agent: I'd be happy to help. Are you using the Acme Smart Home app?\n",
        "Customer: Yes, I have the app on my phone.\n",
        "Agent: Great. Make sure your phone is connected to the 2.4GHz Wi-Fi network, not the 5GHz one.\n",
        "Customer: Oh, I'm on the 5GHz network. Should I switch?\n",
        "Agent: Yes, please switch to the 2.4GHz network. The SmartCam only works with 2.4GHz.\n",
        "Customer: Okay, done. Now what?\n",
        "Agent: Open the app, select 'Add Device', choose 'SmartCam', and follow the on-screen instructions.\n",
        "Customer: It's asking for a password now.\n",
        "Agent: Enter your Wi-Fi password and it should connect.\n",
        "Customer: It's still not working. I keep getting an error message.\n",
        "Agent: I see. In that case, I'd like to escalate this to our technical team. They'll contact you within 24 hours.\n",
        "Customer: Okay, that sounds good. Thank you for trying to help.\n",
        "Agent: You're welcome. Is there anything else you need assistance with?\n",
        "Customer: No, that's all for now. Thanks again.\n",
        "Agent: Thank you for choosing Acme Smart Home. Have a great day!\n",
        "</transcript>\n",
        "\n",
        "<thinking>\n",
        "Main issue: Customer unable to connect new SmartCam to Wi-Fi\n",
        "Resolution: Initial troubleshooting unsuccessful, issue escalated to technical team\n",
        "Follow-up: Required, technical team to contact customer within 24 hours\n",
        "Ambiguities: Specific error message customer is receiving not mentioned\n",
        "</thinking>\n",
        "\n",
        "<json>\n",
        "{\n",
        "  \"summary\": {\n",
        "    \"customerIssue\": \"Unable to connect new SmartCam to Wi-Fi\",\n",
        "    \"resolution\": \"Initial troubleshooting unsuccessful, issue escalated to technical team\",\n",
        "    \"followUpRequired\": true,\n",
        "    \"followUpDetails\": \"Technical team to contact customer within 24 hours for further assistance\"\n",
        "  },\n",
        "  \"status\": \"COMPLETE\",\n",
        "  \"ambiguities\": [\"Specific error message customer is receiving not mentioned\"]\n",
        "}\n",
        "</json>\n",
        "\n",
        "3. Insufficient data:\n",
        "<transcript>\n",
        "Agent: Acme Smart Home Support, this is Sam. How may I assist you?\n",
        "Customer: Hi, my smart lock isn't working.\n",
        "Agent: I'm sorry to hear that. Can you tell me more about the issue?\n",
        "Customer: It just doesn't work. I don't know what else to say.\n",
        "Agent: Okay, when did you first notice the problem? And what model of Acme smart lock do you have?\n",
        "Customer: I don't remember. Listen, I have to go. I'll call back later.\n",
        "Agent: Alright, we're here 24/7 if you need further assistance. Have a good day.\n",
        "</transcript>\n",
        "\n",
        "<thinking>\n",
        "This transcript has fewer than 5 exchanges and the customer's issue is unclear. The customer doesn't provide specific details about the problem with the smart lock or respond to the agent's questions. This interaction doesn't provide sufficient information for a complete summary.\n",
        "</thinking>\n",
        "\n",
        "<json>\n",
        "{\n",
        "  \"status\": \"INSUFFICIENT_DATA\"\n",
        "}\n",
        "</json>\n",
        "</examples>\n",
        "</instructions>\n",
        "\n",
        "Before generating the JSON, please analyze the transcript in <thinking> tags.\n",
        "Include your identification of the main issue, resolution, follow-up requirements, and any ambiguities.\n",
        "Then, provide your JSON output in <json> tags.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "f2PbumFSGdRi"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the role"
      ],
      "metadata": {
        "id": "ZBBkkFcLETSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "setting_the_role = \"\"\"\n",
        "You are an AI assistant specialized in analyzing customer reviews.\n",
        "Your task is to determine the overall sentiment of a given review\n",
        "and extract any specific complaints mentioned.\n",
        "Please follow these instructions carefully:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "WZw8y5Ie7EFE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make It A Prompt Template"
      ],
      "metadata": {
        "id": "66X1-TUeEjX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# llms work better with XMLs\n",
        "\n",
        "instruction_pt1 = \"\"\"\n",
        "1. Review the following customer feedback:\n",
        "\n",
        "<customer_review>\n",
        "{{CUSTOMER_REVIEW}}\n",
        "</customer_review>\n",
        "\"\"\"\n",
        "\n",
        "# <></> to the tell that the start and end of review\n",
        "# {{}} is not a requirement, we can use anything else if we want"
      ],
      "metadata": {
        "id": "Xlu5txQ6EdC2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let the model thing\n",
        "\n",
        "These are the steps which we want our model to go through"
      ],
      "metadata": {
        "id": "C1F8GKqwFNhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instruction_pt2 = \"\"\"\n",
        "2. Analyze the review using the following steps.\n",
        "Show your work in <review_breakdown> tags:\n",
        "\n",
        "a) Key Phrase Extraction:\n",
        "   - Extract and quote key phrases that indicate sentiment\n",
        "   (positive, negative, or neutral).\n",
        "   - Extract and quote key phrases that suggest complaints or issues.\n",
        "\n",
        "b) Sentiment Analysis:\n",
        "   - Consider arguments for positive, negative,\n",
        "   and neutral sentiment based on the extracted phrases.\n",
        "   - Determine the overall sentiment (positive, negative, or neutral)\n",
        "   based on your analysis.\n",
        "   - Explain your reasoning for the sentiment classification.\n",
        "\n",
        "c) Complaint Extraction:\n",
        "   - List each specific issue or problem mentioned in the review.\n",
        "   - For each complaint, provide the relevant quote from the review.\n",
        "   - Count the total number of complaints found.\n",
        "\n",
        "It's OK for this section to be quite long as you\n",
        "thoroughly break down the review.\n",
        "\"\"\"\n",
        "\n",
        "# work will be shown under the review_breakdown tags"
      ],
      "metadata": {
        "id": "j0VMncbkFTi6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output prediction"
      ],
      "metadata": {
        "id": "v-UIsWtJGLD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instruction_pt3 = \"\"\"\n",
        "3. Based on your analysis,\n",
        "generate a JSON output with the following structure:\n",
        "\n",
        "<json>\n",
        "{\n",
        "  \"sentiment_score\": \"Positive|Negative|Neutral\",\n",
        "  \"sentiment_analysis\": \"Explanation of sentiment classification\",\n",
        "  \"complaints\": [\n",
        "    \"Complaint 1\",\n",
        "    \"Complaint 2\",\n",
        "    \"...\"\n",
        "  ]\n",
        "}\n",
        "</json>\n",
        "\n",
        "If no complaints are found,\n",
        "use an empty array for the \"complaints\" field.\n",
        "\n",
        "Remember:\n",
        "- Base your analysis solely on the content of the provided review.\n",
        "- Do not make assumptions or include information\n",
        "not present in the review.\n",
        "- Be objective and focus on the customer's expressed\n",
        "opinions and experiences.\n",
        "- Ensure your JSON output is properly formatted\n",
        "and contains all required fields.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "VleK4ce2F68n"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assembling the Prompt"
      ],
      "metadata": {
        "id": "iWSfgaBxGqfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_prompt = f\"\"\"\n",
        "{setting_the_role}\n",
        "{instruction_pt1}\n",
        "{instruction_pt2}\n",
        "{instruction_pt3}\n",
        "\"\"\"\n",
        "\n",
        "print(final_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TQ-SBy_Gh8o",
        "outputId": "7a535b69-63de-4605-a339-35f7b399dd0d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "You are an AI assistant specialized in analyzing customer reviews. \n",
            "Your task is to determine the overall sentiment of a given review \n",
            "and extract any specific complaints mentioned. \n",
            "Please follow these instructions carefully:\n",
            "\n",
            "\n",
            "1. Review the following customer feedback:\n",
            "\n",
            "<customer_review>\n",
            "{{CUSTOMER_REVIEW}}\n",
            "</customer_review>\n",
            "\n",
            "\n",
            "2. Analyze the review using the following steps. \n",
            "Show your work in <review_breakdown> tags: \n",
            "\n",
            "a) Key Phrase Extraction:\n",
            "   - Extract and quote key phrases that indicate sentiment \n",
            "   (positive, negative, or neutral).\n",
            "   - Extract and quote key phrases that suggest complaints or issues.\n",
            "\n",
            "b) Sentiment Analysis:\n",
            "   - Consider arguments for positive, negative, \n",
            "   and neutral sentiment based on the extracted phrases.\n",
            "   - Determine the overall sentiment (positive, negative, or neutral) \n",
            "   based on your analysis.\n",
            "   - Explain your reasoning for the sentiment classification.\n",
            "\n",
            "c) Complaint Extraction:\n",
            "   - List each specific issue or problem mentioned in the review.\n",
            "   - For each complaint, provide the relevant quote from the review.\n",
            "   - Count the total number of complaints found.\n",
            "\n",
            "It's OK for this section to be quite long as you \n",
            "thoroughly break down the review.\n",
            "\n",
            "\n",
            "3. Based on your analysis, \n",
            "generate a JSON output with the following structure:\n",
            "\n",
            "<json>\n",
            "{\n",
            "  \"sentiment_score\": \"Positive|Negative|Neutral\",\n",
            "  \"sentiment_analysis\": \"Explanation of sentiment classification\",\n",
            "  \"complaints\": [\n",
            "    \"Complaint 1\",\n",
            "    \"Complaint 2\",\n",
            "    \"...\"\n",
            "  ]\n",
            "}\n",
            "</json>\n",
            "\n",
            "If no complaints are found, \n",
            "use an empty array for the \"complaints\" field.\n",
            "\n",
            "Remember:\n",
            "- Base your analysis solely on the content of the provided review.\n",
            "- Do not make assumptions or include information \n",
            "not present in the review.\n",
            "- Be objective and focus on the customer's expressed \n",
            "opinions and experiences.\n",
            "- Ensure your JSON output is properly formatted \n",
            "and contains all required fields.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vision_model= \"llama-3.2-90b-vision-preview\"\n",
        "text_model= \"deepseek-r1-distill-llama-70b\""
      ],
      "metadata": {
        "id": "fTMMxQb6Hdoa"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def get_review_sentiment(review):\n",
        "    #Insert the context into the prompt\n",
        "    prompt = final_prompt.replace(\"{{CUSTOMER_REVIEW}}\", review)\n",
        "\n",
        "    chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        }\n",
        "    ],\n",
        "    model=text_model,\n",
        "    )\n",
        "    output= chat_completion.choices[0].message.content\n",
        "    print(\"ENTIRE MODEL OUTPUT: \")\n",
        "    print(output)\n",
        "\n",
        "    sentiment = re.search(r'<json>(.*?)</json>', output, re.DOTALL)\n",
        "\n",
        "    if sentiment:\n",
        "        print(\"FINAL JSON OUTPUT: \")\n",
        "        print(sentiment.group(1).strip())\n",
        "    else:\n",
        "        print(\"No sentiment analysis in the response.\")"
      ],
      "metadata": {
        "id": "tD6Ehft_Gtrb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing the model"
      ],
      "metadata": {
        "id": "LNCenu4bPtRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review1 = \"\"\"\n",
        "I am in love with my Acme phone.  It's incredible.\n",
        "It's a little expensive, but so worth it imo.\n",
        "If you can afford it, it's worth it!\n",
        "I love the colors too!\n",
        "\"\"\"\n",
        "\n",
        "get_review_sentiment(review1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSA3VWFvISog",
        "outputId": "22ec35b1-753e-4f97-a15a-f85686094a72"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENTIRE MODEL OUTPUT: \n",
            "<think>\n",
            "Okay, so I need to figure out the sentiment of this customer review and extract any complaints. Let me start by reading the review carefully.\n",
            "\n",
            "The customer says, \"I am in love with my Acme phone. It's incredible.\" That sounds really positive. They also mention it's a little expensive but say it's worth it. So they acknowledge the price but don't seem to hold it against the product. They end with loving the colors too, which is another positive point.\n",
            "\n",
            "Looking for key phrases, \"in love\" and \"incredible\" are definitely positive. \"A little expensive\" is a negative point, but they quickly add that it's worth it, so it's more of a neutral or minor complaint. \"Worth it imo\" and \"I love the colors too\" are positive again.\n",
            "\n",
            "For sentiment analysis, the positives far outweigh the negatives. The customer is very happy overall, even though they mention the price. They don't seem to have any major complaints beyond that.\n",
            "\n",
            "As for complaints, the only specific issue is the price being expensive. They don't mention anything else wrong with the product, so that's the only complaint.\n",
            "\n",
            "Putting it all together, the sentiment is positive because the customer is very satisfied despite the price. The complaint is just about the expense, but it's not enough to change the overall positive sentiment.\n",
            "</think>\n",
            "\n",
            "<review_breakdown>\n",
            "a) Key Phrase Extraction:\n",
            "   - Positive sentiment phrases:\n",
            "     - \"I am in love with my Acme phone.\"\n",
            "     - \"It's incredible.\"\n",
            "     - \"so worth it imo.\"\n",
            "     - \"I love the colors too!\"\n",
            "   - Complaints or issues:\n",
            "     - \"It's a little expensive.\"\n",
            "\n",
            "b) Sentiment Analysis:\n",
            "   - Arguments for positive sentiment:\n",
            "     - The customer expresses strong positive emotions (\"I am in love\").\n",
            "     - The customer uses superlatives (\"incredible\").\n",
            "     - The customer believes the product is worth the cost.\n",
            "     - The customer loves the aesthetic aspects (\"colors\").\n",
            "   - Arguments for negative sentiment:\n",
            "     - The customer mentions the product is \"a little expensive.\"\n",
            "   - Overall sentiment:\n",
            "     - Positive\n",
            "   - Reasoning:\n",
            "     - While the customer mentions the product is expensive, they quickly dismiss this concern by stating it's \"so worth it.\" The overwhelming tone of the review is positive, with the customer expressing love and enthusiasm for the product. The negative aspect (expense) is minor compared to the positive aspects.\n",
            "\n",
            "c) Complaint Extraction:\n",
            "   - Specific issues mentioned:\n",
            "     1. \"It's a little expensive.\" (Quote: \"It's a little expensive, but so worth it imo.\")\n",
            "   - Total number of complaints: 1\n",
            "</review_breakdown>\n",
            "\n",
            "<json>\n",
            "{\n",
            "  \"sentiment_score\": \"Positive\",\n",
            "  \"sentiment_analysis\": \"The review is overwhelmingly positive. The customer expresses love and enthusiasm for the product, and while they mention it is expensive, they quickly dismiss this concern by stating it is worth the cost. The negative aspect (expense) is minor compared to the positive aspects (incredible performance, worth the cost, and love for the colors).\",\n",
            "  \"complaints\": [\n",
            "    \"It's a little expensive.\"\n",
            "  ]\n",
            "}\n",
            "</json>\n",
            "FINAL JSON OUTPUT: \n",
            "{\n",
            "  \"sentiment_score\": \"Positive\",\n",
            "  \"sentiment_analysis\": \"The review is overwhelmingly positive. The customer expresses love and enthusiasm for the product, and while they mention it is expensive, they quickly dismiss this concern by stating it is worth the cost. The negative aspect (expense) is minor compared to the positive aspects (incredible performance, worth the cost, and love for the colors).\",\n",
            "  \"complaints\": [\n",
            "    \"It's a little expensive.\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review2 = \"\"\"\n",
        "I recently bought the ABC Smartwatch, and I have to say, it's been a\n",
        "bit of a rollercoaster. Let's start with the good stuff -\n",
        "the design is absolutely gorgeous. It's sleek, lightweight,\n",
        "and looks fantastic on my wrist. The display is crisp and bright,\n",
        "even in direct sunlight. I also love how customizable the watch\n",
        "faces are, allowing me to switch up the look whenever I want.\n",
        "However, there are some significant downsides that I can't ignore.\n",
        "The battery life is terrible - I'm lucky if I get through a full\n",
        "day without needing to charge it. This is especially frustrating\n",
        "when I'm traveling or out for long periods. The fitness tracking\n",
        "features, which were a big selling point for me, are hit or miss.\n",
        "It often doesn't accurately count my steps or calculate calories burned\n",
        "during workouts. Another issue I've encountered is with the touch\n",
        "sensitivity. Sometimes it's overly sensitive, registering accidental\n",
        "touches, while other times I have to tap multiple times for it to\n",
        "respond. It's inconsistent and annoying, especially when I'm trying to\n",
        "quickly check notifications or start a workout.On the plus side, the\n",
        "integration with my smartphone is seamless, and I appreciate being able\n",
        "to respond to texts and calls from my wrist. The water resistance has\n",
        "also held up well - I've worn it while swimming without any problems.\n",
        "Customer service has been decent. They were quick to respond when I\n",
        "reported the battery issue, but their solution of turning off certain\n",
        "features defeats the purpose of having a smartwatch. All in all, while\n",
        "there are aspects of the ABC Smartwatch that I really like, the battery\n",
        "life and inconsistent performance are major drawbacks. For the price\n",
        "point, I expected better. It's not worth your money.\n",
        "\"\"\"\n",
        "\n",
        "get_review_sentiment(review2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdmrpAYqPlFn",
        "outputId": "cee7b3cc-e222-496c-dedc-d114a51117e5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENTIRE MODEL OUTPUT: \n",
            "<think>\n",
            "Okay, so I need to analyze this customer review of the ABC Smartwatch. Let me start by reading through the review carefully to understand the customer's experience.\n",
            "\n",
            "The review begins by mentioning that the experience has been a \"rollercoaster,\" which suggests there are both positive and negative aspects. The customer starts with the positives: they love the design, calling it \"absolutely gorgeous,\" \"sleek,\" and \"lightweight.\" They also appreciate the crisp and bright display, especially in sunlight, and the customizable watch faces. These are all strong positive points.\n",
            "\n",
            "Then, the customer shifts to the negatives. They mention that the battery life is \"terrible,\" only lasting a day, which is a significant issue, especially when traveling. Next, the fitness tracking features are \"hit or miss,\" with inaccurate step counts and calorie calculations. This is a big problem since it was a key selling point for them. They also talk about inconsistent touch sensitivity, sometimes too sensitive and other times unresponsive, which is annoying when trying to use the watch quickly.\n",
            "\n",
            "There are some more positives: seamless smartphone integration for responding to texts and calls, and good water resistance. However, the customer service response to the battery issue was disappointing, as the solution suggested disabling features, which defeats the purpose of a smartwatch.\n",
            "\n",
            "Overall, the customer concludes that while there are things they like, the battery life and performance issues are major drawbacks, especially for the price. They don't recommend the product, feeling it's not worth the money.\n",
            "\n",
            "Now, breaking this down:\n",
            "\n",
            "For key phrases, I'll extract both positive and negative sentiments. Positive phrases include the design, display, customization, smartphone integration, and water resistance. Negative phrases include battery life, inaccurate fitness tracking, touch sensitivity issues, and poor customer service solution.\n",
            "\n",
            "Sentiment-wise, the review has both positive and negative points, but the negative issues seem to outweigh the positives. The customer explicitly states that the drawbacks make the product not worth buying, which leans the overall sentiment to negative.\n",
            "\n",
            "Complaints are specific issues: battery life, fitness tracking inaccuracies, touch sensitivity problems, and poor customer service response. That's four complaints in total.\n",
            "\n",
            "So, the JSON should reflect a negative sentiment, list the complaints, and provide a brief explanation of the sentiment analysis.\n",
            "</think>\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"sentiment_score\": \"Negative\",\n",
            "  \"sentiment_analysis\": \"The review expresses significant dissatisfaction with the product, primarily due to battery life issues, inconsistent performance, and poor customer service. Despite positive aspects like design and features, the major drawbacks lead to an overall negative sentiment.\",\n",
            "  \"complaints\": [\n",
            "    \"Terrible battery life\",\n",
            "    \"Inaccurate fitness tracking\",\n",
            "    \"Inconsistent touch sensitivity\",\n",
            "    \"Poor customer service response\"\n",
            "  ]\n",
            "}\n",
            "```\n",
            "No sentiment analysis in the response.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N shot prompting\n",
        "\n",
        "When the model is not performing well then we give example to show model how to work"
      ],
      "metadata": {
        "id": "nczKnczzQTy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = \"\"\"\n",
        "Here are some example inputs and outputs to help you understand\n",
        "the sort of analysis we're looking for\n",
        "<examples>\n",
        "<example1>\n",
        "\n",
        "<customer_review>\n",
        "I recently bought the ABC Smartwatch, and I have to say, it's been a bit of a rollercoaster. Let's start with the good stuff - the design is absolutely gorgeous. It's sleek, lightweight, and looks fantastic on my wrist. The display is crisp and bright, even in direct sunlight. I also love how customizable the watch faces are, allowing me to switch up the look whenever I want.\n",
        "However, there are some significant downsides that I can't ignore. The battery life is terrible - I'm lucky if I get through a full day without needing to charge it. This is especially frustrating when I'm traveling or out for long periods. The fitness tracking features, which were a big selling point for me, are hit or miss. It often doesn't accurately count my steps or calculate calories burned during workouts.\n",
        "Another issue I've encountered is with the touch sensitivity. Sometimes it's overly sensitive, registering accidental touches, while other times I have to tap multiple times for it to respond. It's inconsistent and annoying, especially when I'm trying to quickly check notifications or start a workout.\n",
        "On the plus side, the integration with my smartphone is seamless, and I appreciate being able to respond to texts and calls from my wrist. The water resistance has also held up well - I've worn it while swimming without any problems.\n",
        "Customer service has been decent. They were quick to respond when I reported the battery issue, but their solution of turning off certain features defeats the purpose of having a smartwatch.\n",
        "All in all, while there are aspects of the ABC Smartwatch that I really like, the battery life and inconsistent performance are major drawbacks. For the price point, I expected better. It's not worth your money.\n",
        "</customer_review>\n",
        "\n",
        "<ideal_output>\n",
        "<review_analysis>\n",
        "a) Key Phrase Extraction:\n",
        "\n",
        "Positive Phrases:\n",
        "1. \"design is absolutely gorgeous\"\n",
        "2. \"sleek, lightweight, and looks fantastic\"\n",
        "3. \"display is crisp and bright\"\n",
        "4. \"love how customizable the watch faces are\"\n",
        "5. \"integration with my smartphone is seamless\"\n",
        "6. \"water resistance has held up well\"\n",
        "7. \"customer service has been decent\"\n",
        "\n",
        "Negative Phrases:\n",
        "1. \"bit of a rollercoaster\"\n",
        "2. \"battery life is terrible\"\n",
        "3. \"lucky if I get through a full day\"\n",
        "4. \"fitness tracking features are hit or miss\"\n",
        "5. \"doesn't accurately count my steps\"\n",
        "6. \"touch sensitivity... overly sensitive\"\n",
        "7. \"have to tap multiple times\"\n",
        "8. \"inconsistent and annoying\"\n",
        "9. \"solution of turning off certain features defeats the purpose\"\n",
        "10. \"battery life and inconsistent performance are major drawbacks\"\n",
        "11. \"not worth your money\"\n",
        "\n",
        "b) Sentiment Analysis:\n",
        "Arguments for positive sentiment:\n",
        "- The design and aesthetics of the watch are highly praised\n",
        "- The display quality is commended\n",
        "- Customization options are appreciated\n",
        "- Smartphone integration works well\n",
        "- Water resistance feature is effective\n",
        "- Customer service responded quickly\n",
        "\n",
        "Arguments for negative sentiment:\n",
        "- Battery life is severely criticized\n",
        "- Fitness tracking, a key feature, is unreliable\n",
        "- Touch sensitivity issues cause frustration\n",
        "- The proposed solution compromises functionality\n",
        "- The price doesn't justify the performance\n",
        "- The reviewer explicitly states it's not worth the money\n",
        "\n",
        "Considering the strength and frequency of sentiments:\n",
        "- Positive sentiments are mostly about aesthetics and basic functions\n",
        "- Negative sentiments relate to core functionalities and user experience\n",
        "- The reviewer uses stronger language for negative aspects (\"terrible\", \"major drawbacks\", \"not worth your money\")\n",
        "- The conclusion emphasizes disappointment and recommends against purchase\n",
        "\n",
        "Based on this analysis, the overall sentiment leans negative. While the reviewer appreciates certain aspects, the core issues with battery life, performance, and value for money overshadow the positives. The explicit recommendation against purchase is a strong indicator of overall negative sentiment.\n",
        "\n",
        "c) Complaint Extraction:\n",
        "1. Poor battery life\n",
        "   Quote: \"The battery life is terrible - I'm lucky if I get through a full day without needing to charge it\"\n",
        "\n",
        "2. Inaccurate fitness tracking\n",
        "   Quote: \"The fitness tracking features, which were a big selling point for me, are hit or miss. It often doesn't accurately count my steps or calculate calories burned during workouts\"\n",
        "\n",
        "3. Inconsistent touch sensitivity\n",
        "   Quote: \"Sometimes it's overly sensitive, registering accidental touches, while other times I have to tap multiple times for it to respond\"\n",
        "\n",
        "4. Inadequate solution to problems\n",
        "   Quote: \"their solution of turning off certain features defeats the purpose of having a smartwatch\"\n",
        "\n",
        "Total Complaints: 4\n",
        "\n",
        "d) Summary:\n",
        "The analysis of the ABC Smartwatch review reveals a predominantly negative sentiment. While the customer appreciates the design, display quality, and some basic features, these positives are outweighed by significant issues with core functionalities. The main complaints center around poor battery life, inaccurate fitness tracking, inconsistent touch sensitivity, and inadequate problem-solving from customer service. The reviewer concludes that the product is not worth the money, emphasizing that the drawbacks are too significant to justify the price point.\n",
        "</review_analysis>\n",
        "\n",
        "<json>\n",
        "{\n",
        "  \"sentiment_score\": \"Negative\",\n",
        "  \"sentiment_analysis\": \"While the review acknowledges positive aspects like design and basic functionality, the overall sentiment is negative due to significant issues with core features (battery life, fitness tracking, touch sensitivity). The reviewer explicitly states it's 'not worth your money' and emphasizes major drawbacks that overshadow the positive elements.\",\n",
        "  \"complaints\": [\n",
        "    \"Poor battery life requiring daily charging\",\n",
        "    \"Inaccurate fitness tracking features\",\n",
        "    \"Inconsistent touch sensitivity causing user interface problems\",\n",
        "    \"Inadequate customer service solutions that compromise device functionality\"\n",
        "  ]\n",
        "}\n",
        "</json>\n",
        "</ideal_output>\n",
        "\n",
        "</example1>\n",
        "</examples>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "I7hKnLOHP2qP"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eRjJPjvOQd2K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}