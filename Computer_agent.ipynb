{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q llama_index llama-index-llms-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0_sWGpYZQLl",
        "outputId": "4ab0a6f0-f565-48d2-85ad-9329b3e377b5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.6/250.6 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.7/298.7 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install llama-index-vector-stores-qdrant llama-index-readers-file llama-index-embeddings-fastembed\n",
        "!pip install -q youtube-transcript-api langchain_community tiktoken langchain-openai langchainhub chromadb langchain langchain-core langchain_google_genai llama_index qdrant-client sentence-transformers fastembed llama-index-llms-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RgECA44JbSsf",
        "outputId": "84142666-e0d5-49b1-80ba-456884ff59ec"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index-vector-stores-qdrant\n",
            "  Downloading llama_index_vector_stores_qdrant-0.4.3-py3-none-any.whl.metadata (767 bytes)\n",
            "Requirement already satisfied: llama-index-readers-file in /usr/local/lib/python3.11/dist-packages (0.4.4)\n",
            "Collecting llama-index-embeddings-fastembed\n",
            "  Downloading llama_index_embeddings_fastembed-0.3.0-py3-none-any.whl.metadata (697 bytes)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.60.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-vector-stores-qdrant) (1.70.0)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.7 in /usr/local/lib/python3.11/dist-packages (from llama-index-vector-stores-qdrant) (0.12.15)\n",
            "Collecting qdrant-client>=1.7.1 (from llama-index-vector-stores-qdrant)\n",
            "  Downloading qdrant_client-1.13.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file) (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file) (2.2.2)\n",
            "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file) (5.2.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file) (0.0.26)\n",
            "Collecting fastembed>=0.2.2 (from llama-index-embeddings-fastembed)\n",
            "  Downloading fastembed-0.5.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file) (2.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.20 in /usr/local/lib/python3.11/dist-packages (from fastembed>=0.2.2->llama-index-embeddings-fastembed) (0.27.1)\n",
            "Collecting loguru<0.8.0,>=0.7.2 (from fastembed>=0.2.2->llama-index-embeddings-fastembed)\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting mmh3<5.0.0,>=4.1.0 (from fastembed>=0.2.2->llama-index-embeddings-fastembed)\n",
            "  Downloading mmh3-4.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.11/dist-packages (from fastembed>=0.2.2->llama-index-embeddings-fastembed) (1.26.4)\n",
            "Collecting onnxruntime!=1.20.0,>=1.17.0 (from fastembed>=0.2.2->llama-index-embeddings-fastembed)\n",
            "  Downloading onnxruntime-1.20.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting pillow<11.0.0,>=10.3.0 (from fastembed>=0.2.2->llama-index-embeddings-fastembed)\n",
            "  Downloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting py-rust-stemmers<0.2.0,>=0.1.0 (from fastembed>=0.2.2->llama-index-embeddings-fastembed)\n",
            "  Downloading py_rust_stemmers-0.1.3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: requests<3.0,>=2.31 in /usr/local/lib/python3.11/dist-packages (from fastembed>=0.2.2->llama-index-embeddings-fastembed) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from fastembed>=0.2.2->llama-index-embeddings-fastembed) (0.21.0)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.66 in /usr/local/lib/python3.11/dist-packages (from fastembed>=0.2.2->llama-index-embeddings-fastembed) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (3.11.11)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (3.9.1)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (2.10.6)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (9.0.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (0.8.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.17.2)\n",
            "Collecting grpcio-tools>=1.41.0 (from qdrant-client>=1.7.1->llama-index-vector-stores-qdrant)\n",
            "  Downloading grpcio_tools-1.70.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant-client>=1.7.1->llama-index-vector-stores-qdrant)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.11/dist-packages (from qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (2.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file) (2025.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.18.3)\n",
            "Collecting protobuf<6.0dev,>=5.26.1 (from grpcio-tools>=1.41.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant)\n",
            "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from grpcio-tools>=1.41.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (75.1.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (0.14.0)\n",
            "Collecting h2<5,>=3 (from httpx[http2]>=0.20.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant)\n",
            "  Downloading h2-4.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed>=0.2.2->llama-index-embeddings-fastembed) (3.17.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed>=0.2.2->llama-index-embeddings-fastembed) (24.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (2024.11.6)\n",
            "Collecting coloredlogs (from onnxruntime!=1.20.0,>=1.17.0->fastembed>=0.2.2->llama-index-embeddings-fastembed)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed>=0.2.2->llama-index-embeddings-fastembed) (25.1.24)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed>=0.2.2->llama-index-embeddings-fastembed) (1.13.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.31->fastembed>=0.2.2->llama-index-embeddings-fastembed) (3.4.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (3.26.1)\n",
            "Collecting hyperframe<7,>=6.1 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant)\n",
            "  Downloading hyperframe-6.1.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting hpack<5,>=4.1 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant)\n",
            "  Downloading hpack-4.1.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.7->llama-index-vector-stores-qdrant) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime!=1.20.0,>=1.17.0->fastembed>=0.2.2->llama-index-embeddings-fastembed)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime!=1.20.0,>=1.17.0->fastembed>=0.2.2->llama-index-embeddings-fastembed) (1.3.0)\n",
            "Downloading llama_index_vector_stores_qdrant-0.4.3-py3-none-any.whl (11 kB)\n",
            "Downloading llama_index_embeddings_fastembed-0.3.0-py3-none-any.whl (2.7 kB)\n",
            "Downloading fastembed-0.5.1-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qdrant_client-1.13.2-py3-none-any.whl (306 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.6/306.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio_tools-1.70.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-4.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.20.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Downloading py_rust_stemmers-0.1.3-cp311-cp311-manylinux_2_28_x86_64.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.9/323.9 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-4.2.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-4.1.0-py3-none-any.whl (34 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hyperframe-6.1.0-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: py-rust-stemmers, mmh3, protobuf, portalocker, pillow, loguru, hyperframe, humanfriendly, hpack, h2, grpcio-tools, coloredlogs, onnxruntime, qdrant-client, fastembed, llama-index-vector-stores-qdrant, llama-index-embeddings-fastembed\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.6\n",
            "    Uninstalling protobuf-4.25.6:\n",
            "      Successfully uninstalled protobuf-4.25.6\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.1.0\n",
            "    Uninstalling pillow-11.1.0:\n",
            "      Successfully uninstalled pillow-11.1.0\n",
            "Successfully installed coloredlogs-15.0.1 fastembed-0.5.1 grpcio-tools-1.70.0 h2-4.2.0 hpack-4.1.0 humanfriendly-10.0 hyperframe-6.1.0 llama-index-embeddings-fastembed-0.3.0 llama-index-vector-stores-qdrant-0.4.3 loguru-0.7.3 mmh3-4.1.0 onnxruntime-1.20.1 pillow-10.4.0 portalocker-2.10.1 protobuf-5.29.3 py-rust-stemmers-0.1.3 qdrant-client-1.13.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "b67f82b070cc4c7d8bab210deb0144a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.3/622.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.7/412.7 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up LLM"
      ],
      "metadata": {
        "id": "JHmfYX1BZLtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kx6BwlAdyBP",
        "outputId": "ffd487cb-6f0f-4ed6-c76f-0b629057fb3d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n",
            "Downloading groq-0.18.0-py3-none-any.whl (121 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/121.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from llama_index.llms.groq import Groq\n",
        "from llama_index.core import Document\n",
        "import uuid\n",
        "from llama_index.core.text_splitter import SentenceSplitter\n",
        "import logging\n",
        "import sys\n",
        "import os\n",
        "import qdrant_client\n",
        "from IPython.display import Markdown, display\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.core import StorageContext\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
        "from llama_index.core import Settings\n"
      ],
      "metadata": {
        "id": "HMSL1vqxd3Hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "aisXK2ECYp8g"
      },
      "outputs": [],
      "source": [
        "from groq import Groq\n",
        "import os\n",
        "\n",
        "\n",
        "client = Groq(\n",
        "    api_key=os.environ.get(\"groq_api\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Generate a numbered, ordered list of technical topics I should learn if I want to work on LLM\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"deepseek-r1-distill-llama-70b\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnQG0swEcQmU",
        "outputId": "83046d6a-d0e9-49c8-965c-313a2814859c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<think>\n",
            "Okay, so I want to work on large language models (LLMs), but I'm not exactly sure where to start. I've heard that LLMs are a big part of AI these days, and they're used for things like chatbots, translation, and text generation. But I'm a bit overwhelmed by all the technical terms and areas I need to learn. Let me try to break this down.\n",
            "\n",
            "First, I know that LLMs are a type of machine learning model, specifically deep learning. So, I guess I need to start with the basics of machine learning. That probably includes supervised and unsupervised learning. I've heard about neural networks, so I should look into that. Maybe I should understand how neural networks work, the different layers like input, hidden, and output layers. Also, activation functions and how they introduce non-linearity into the model.\n",
            "\n",
            "Wait, optimization techniques are also important. I remember hearing about gradient descent and backpropagation. These are methods used to train models, right? So I need to understand how those work. Maybe I should also look into different optimizers like Adam or SGD. Oh, and loss functions, because that's what the model tries to minimize during training.\n",
            "\n",
            "Then there's deep learning frameworks. I know TensorFlow and PyTorch are popular. I think PyTorch is more commonly used in research, so maybe I should focus on that. I should learn how to build models, train them, and use pre-trained models. Also, I need to understand how GPUs work with these frameworks because training models can be computationally intensive.\n",
            "\n",
            "Moving on to natural language processing (NLP), which is the area where LLMs are primarily applied. I think I should start with the basics of NLP. Tokenization is the process of breaking text into words or tokens, right? Then there's stop words removal, stemming, and lemmatization. Oh, and understanding embeddings like Word2Vec or GloVe, which convert words into vectors.\n",
            "\n",
            "Sequence models are next. I've heard of RNNs, LSTMs, and GRUs. RNNs are good for sequential data, but they have issues with long-term dependencies. LSTMs and GRUs are improvements over RNNs, so I should learn about their architectures and how they handle memory.\n",
            "\n",
            "Attention mechanisms are crucial for models like Transformers. I remember the Transformer model introduced self-attention, which allows the model to focus on different parts of the input when generating each part of the output. I need to understand how multi-head attention works and the concept of positional encoding since Transformers don't have recurrence.\n",
            "\n",
            "Pre-trained language models are a big part of LLMs. I should learn about models like BERT, RoBERTa, and GPT. Each has a different approach—BERT uses a masked language modeling approach, while GPT is autoregressive. Understanding how these models are trained and fine-tuned is important.\n",
            "\n",
            "For LLMs specifically, I need to know about their architecture. They're usually based on the Transformer model, so I should dive deeper into that. Scaling is another aspect—models have gotten larger, so understanding how model size affects performance and training is key.\n",
            "\n",
            "Training LLMs must require a lot of data and computational resources. I should learn about the datasets used, like the Common Crawl or BookCorpus. Data preprocessing is also important—how to clean and format the data for training. Distributed training is probably necessary for large models, so I need to understand how that works, maybe using libraries like Dask or joblib.\n",
            "\n",
            "Once the model is trained, evaluation metrics are needed. For LLMs, perplexity is a common metric, but I'm not exactly sure how it's calculated. I should also look into other metrics like BLEU or ROUGE, which are used for text generation tasks.\n",
            "\n",
            "Fine-tuning is the process of adapting a pre-trained model to a specific task. I need to learn how to do that effectively, maybe starting with smaller models before scaling up. Prompt engineering is something I've heard about recently, where you design prompts to guide the model's output. That seems important for getting the desired results from the model.\n",
            "\n",
            "Advanced topics like few-shot and zero-shot learning would be useful. Few-shot learning means the model can learn from a few examples, which is different from traditional machine learning approaches. Zero-shot learning is when the model can perform tasks it wasn't explicitly trained on, which sounds pretty advanced.\n",
            "\n",
            "Reinforcement learning from human feedback (RLHF) is another area. I think this involves using human feedback to fine-tune the model's outputs, making them more aligned with what users want. I need to understand how this process works and how it's integrated into training.\n",
            "\n",
            "Ethical considerations are crucial. I should learn about bias in models and how to mitigate it. Also, ensuring that the model doesn't generate harmful content is important. Environmental impact is another aspect, considering the energy required to train large models.\n",
            "\n",
            "Specialized areas within LLMs might interest me. For example, multimodal models that can handle text and images, or models designed for specific tasks like code generation or question answering. I should explore these areas if I have particular interests.\n",
            "\n",
            "Staying updated with the latest research is important. Reading papers from conferences like NeurIPS or ACL would help me keep up with advancements. Maybe joining communities or following experts on social media could provide additional insights and resources.\n",
            "\n",
            "Finally, building projects is a great way to apply what I've learned. Starting with simple models and gradually moving to more complex ones would help solidify my understanding. Contributing to open-source projects could also provide practical experience and expose me to real-world challenges.\n",
            "\n",
            "Wait, I should make sure I have a strong foundation in programming, especially Python, since it's widely used in machine learning. Also, understanding software engineering best practices will help when working on larger projects or contributing to existing ones.\n",
            "\n",
            "I think I've covered a lot of areas, but I might be missing some details. Maybe I should organize this into a structured list to make sure I don't forget anything important. Starting from the basics and moving to more advanced topics seems logical. I should also consider practical experience, like setting up a machine with the right tools and libraries, and experimenting with existing models to get a feel for how they work.\n",
            "\n",
            "Overall, it's a lot to learn, but breaking it down into manageable sections makes it less daunting. I can tackle each topic one by one, starting with the basics and gradually moving to more specialized areas. Practicing regularly and working on projects will help reinforce the concepts and prepare me for working on LLMs.\n",
            "</think>\n",
            "\n",
            "To work on Large Language Models (LLMs), follow this organized and structured approach:\n",
            "\n",
            "### 1. **Foundational Knowledge**\n",
            "   - **Machine Learning Basics**: Understand supervised and unsupervised learning, neural networks, activation functions, and optimization techniques like gradient descent and backpropagation.\n",
            "   - **Deep Learning Frameworks**: Focus on PyTorch for building, training, and using pre-trained models, and understand GPU utilization.\n",
            "\n",
            "### 2. **Natural Language Processing (NLP) Basics**\n",
            "   - **Text Preprocessing**: Learn tokenization, stop words removal, stemming, and lemmatization.\n",
            "   - **Word Embeddings**: Study Word2Vec, GloVe, and their applications.\n",
            "   - **Sequence Models**: Understand RNNs, LSTMs, GRUs, and their role in sequential data.\n",
            "   - **Attention Mechanisms**: Delve into self-attention, multi-head attention, and positional encoding.\n",
            "\n",
            "### 3. **Pre-trained Language Models**\n",
            "   - **Models Overview**: Learn about BERT, RoBERTa, and GPT, focusing on their training methods and fine-tuning.\n",
            "\n",
            "### 4. **Large Language Models (LLMs)**\n",
            "   - **Architecture**: Study the Transformer model and scaling aspects.\n",
            "   - **Training**: Understand data sources like Common Crawl, preprocessing, and distributed training.\n",
            "   - **Evaluation**: Familiarize yourself with perplexity, BLEU, and ROUGE.\n",
            "   - **Fine-tuning and Prompt Engineering**: Adapt models to specific tasks and design effective prompts.\n",
            "\n",
            "### 5. **Advanced Topics**\n",
            "   - **Few-shot and Zero-shot Learning**: Explore learning from minimal examples and unseen tasks.\n",
            "   - **Reinforcement Learning from Human Feedback (RLHF)**: Use human feedback to refine model outputs.\n",
            "   - **Ethical Considerations**: Address bias mitigation, harmful content prevention, and environmental impact.\n",
            "\n",
            "### 6. **Specialized Areas**\n",
            "   - **Multimodal Models**: Explore models handling text and images.\n",
            "   - **Task-specific Models**: Investigate models for code generation or question answering.\n",
            "\n",
            "### 7. **Community and Research**\n",
            "   - **Stay Updated**: Read research papers and join communities for the latest advancements.\n",
            "\n",
            "### 8. **Practical Experience**\n",
            "   - **Projects**: Start with simple models, progress to complex ones, and contribute to open-source projects.\n",
            "   - **Tools and Programming**: Ensure proficiency in Python and software engineering practices.\n",
            "\n",
            "By systematically addressing each area, you can build a comprehensive understanding and practical skills needed to work on LLMs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Hello! You are an assistant who will only reply in Roman urdu language\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Kia haal hai? Main aapki kaise madad kar sakta hoon?\"},\n",
        "        {\"role\": \"user\", \"content\": \"Create a well-organized routine for me\"}\n",
        "    ],\n",
        "    model=\"deepseek-r1-distill-llama-70b\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O62FXc8OcdDh",
        "outputId": "7e3c8d5e-5433-43b8-a00a-592b4b8faeed"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<think>\n",
            "Alright, the user just asked me to create a well-organized routine for them. They mentioned they want it in Roman Urdu, so I need to make sure I structure it clearly in that format. \n",
            "\n",
            "First, I should consider the key components of a daily routine. Usually, it starts from waking up, then exercise, breakfast, work or study, lunch, some leisure time, more work, evening activities, dinner, and then winding down before bed. \n",
            "\n",
            "I need to break down each part of the day into manageable chunks. Maybe allocate specific time slots so the routine is structured. Let's see, waking up at 6 AM is a good start, then exercise until 7. After that, breakfast and getting ready. \n",
            "\n",
            "For work or study, I can set aside a solid block in the morning. Lunch at 1 PM, then some rest. The afternoon can be for more work or studies. Leisure time in the evening, maybe around 5 PM. Dinner at 8 PM and some relaxation before bed at 10:30 PM. \n",
            "\n",
            "I should also include some tips to make the routine effective. Maybe advice on sticking to the schedule, taking short breaks, staying hydrated, eating healthy, exercising regularly, and getting enough sleep. \n",
            "\n",
            "I need to present all this in Roman Urdu, making sure it's easy to read and follow. Each time slot should be clear, and the tips should be concise but helpful. \n",
            "\n",
            "Let me organize this into sections: Morning, Day, Evening, Night, and then the tips. That way, it's easy for the user to follow. I'll use bullet points or numbers where necessary to enhance readability. \n",
            "\n",
            "I should also make sure the language is simple and straightforward, avoiding any complicated words since the user might not be highly fluent in English. \n",
            "\n",
            "Double-checking the time allocations to ensure they are realistic and allow for a balanced day. It's important the routine isn't too rigid but still provides structure. \n",
            "\n",
            "Including elements like leisure time and relaxation is crucial to maintain a healthy work-life balance. Maybe suggest activities like reading or walking in the evening. \n",
            "\n",
            "Alright, putting it all together now, ensuring each part flows logically and covers all aspects of the day. Making sure the Roman Urdu is accurate and easy to understand. \n",
            "\n",
            "I think that's a solid plan. Time to draft the response accordingly.\n",
            "</think>\n",
            "\n",
            "Yeh raha aapka well-organized daily routine (Roman Urdu mein):\n",
            "\n",
            "---\n",
            "\n",
            "### **Morning Routine (Subah Ki Routine):**\n",
            "1. **6:00 AM - Wake Up (Jagna):**\n",
            "   - Bed se uthkar duaa karein.\n",
            "   - Morning walk ya exercise karein (30 minutes).\n",
            "\n",
            "2. **6:30 AM - Fresh Up (Tayyar Hona):**\n",
            "   - Brush karein, face wash karein, aur naha dhokar fresh ho jayein.\n",
            "\n",
            "3. **7:00 AM - Breakfast (Nashta):**\n",
            "   - Healthy breakfast karein (jaise poha, omelette, ya phal).\n",
            "\n",
            "4. **7:30 AM - Get Ready (Tayyar Hona):**\n",
            "   - Office ya studies ke liye tayyar ho jayein.\n",
            "\n",
            "---\n",
            "\n",
            "### **Day Routine (Din Ki Routine):**\n",
            "5. **8:00 AM - Start Work/Study (Kaam ya Padhai Shuru Karein):**\n",
            "   - Apne goals ko achieve karne ke liye focused rahein.\n",
            "   - Short breaks lein (5-10 minutes) har 1 ghante ke baad.\n",
            "\n",
            "6. **1:00 PM - Lunch (Dopahar Ka Khana):**\n",
            "   - Healthy aur poora khana karein.\n",
            "   - 10-15 minutes ka rest lein.\n",
            "\n",
            "7. **1:30 PM - Resume Work/Study (Kaam ya Padhai Dobara Shuru Karein):**\n",
            "   - Agla session focused aur productive banayein.\n",
            "\n",
            "8. **5:00 PM - Evening Break (Sham Ki Chai):**\n",
            "   - Chai ya coffee karein, aur thoda samay relax karein.\n",
            "\n",
            "---\n",
            "\n",
            "### **Evening Routine (Sham Ki Routine):**\n",
            "9. **5:30 PM - Leisure Time (Aaram Karne Ka Samay):**\n",
            "   - Kuchh samay apne shauk ke liye nikaalein (jaise kitaab padhna, gaana sunna, ya walk karna).\n",
            "\n",
            "10. **6:30 PM - Exercise/Yoga (Vyaayam ya Yoga):**\n",
            "    - Rozana 30-45 minutes exercise karein.\n",
            "\n",
            "11. **7:30 PM - Dinner (Shaam Ka Khana):**\n",
            "    - Healthy aur halka khana karein.\n",
            "\n",
            "---\n",
            "\n",
            "### **Night Routine (Raat Ki Routine):**\n",
            "12. **8:00 PM - Relaxation Time (Aaram Karne Ka Samay):**\n",
            "    - Family ya doston ke saath samay bitayein.\n",
            "    - TV dekhna ya koi hobby pursue karein.\n",
            "\n",
            "13. **9:30 PM - Wind Down (Din Bhar Ka End):**\n",
            "    - Apne din ki planning aur goals review karein.\n",
            "    - Kal ke liye plan banayein.\n",
            "\n",
            "14. **10:00 PM - Prepare for Bed (Bistar Tyaar Karein):**\n",
            "    - Light reading ya meditation karein.\n",
            "\n",
            "15. **10:30 PM - Sleep (Sone Jaayein):**\n",
            "    - Puray 7-8 ghante ki neend lein.\n",
            "\n",
            "---\n",
            "\n",
            "### **Tips for a Successful Routine:**\n",
            "- Har din is routine ka strictly paalan karein.\n",
            "- Short breaks lete rahein taaki dimaag fresh rahe.\n",
            "- Paani peena aur healthy khana khayein.\n",
            "- Exercise ko apni routine ka hissa banayein.\n",
            "- Neend ka pura dhyan rakhein.\n",
            "\n",
            "Umeed hai yeh routine aapko organized aur productive banayegi! 😊\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CJHqwPlqflVJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}